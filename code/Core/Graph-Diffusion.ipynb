{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solutions:\n",
    "\n",
    "propagation asyn.: cnt->concat(features,ngh structure)\n",
    "\n",
    "conservation: normalize as the tot in local/global\n",
    "\n",
    "propagation dec.: dis&time->dec function\n",
    "\n",
    "local structure feature(sample+global vec)\n",
    "\n",
    "kernel methods\n",
    "\n",
    "# asynchronous propagation\n",
    "def asynchronous_prop():\n",
    "\n",
    "# local strucuture sample\n",
    "def local_structure_sample():\n",
    "\n",
    "# propagation descent\n",
    "def propgation_heat_descent():\n",
    "\n",
    "# k-hop interaction\n",
    "def k_hop_interaction():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from scipy.sparse import csgraph\n",
    "import scipy.sparse.linalg\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_HK(A, alpha: float, eps: float):\n",
    "    N = A.shape[0]\n",
    "\n",
    "    # Self-loops\n",
    "    A_loop = sp.eye(N) + A\n",
    "\n",
    "    # Symmetric transition matrix\n",
    "    D_loop_vec = A_loop.sum(0).A1\n",
    "    D_loop_vec_invsqrt = 1 / np.sqrt(D_loop_vec)\n",
    "    D_loop_vec_invsqrt[np.isinf(D_loop_vec_invsqrt)] = 0.\n",
    "    D_loop_invsqrt = sp.diags(D_loop_vec_invsqrt)\n",
    "    T_sym = A.dot(D_loop_invsqrt).transpose().dot(D_loop_invsqrt).tocoo()#D_loop_invsqrt @ A_loop @ D_loop_invsqrt\n",
    "\n",
    "    # PPR-based diffusion\n",
    "    S = alpha * sp.linalg.inv(sp.eye(N) - (1 - alpha) * T_sym)\n",
    "\n",
    "    # Sparsify using threshold epsilon\n",
    "    S_tilde = S.multiply(S >= eps)\n",
    "\n",
    "    # Column-normalized transition matrix on graph S_tilde\n",
    "    D_tilde_vec = S_tilde.sum(0).A1\n",
    "    T_S = S_tilde / D_tilde_vec\n",
    "\n",
    "    T_S = scipy.sparse.csr_matrix(T_S)\n",
    "\n",
    "    print(\"T_S.shape\",T_S.shape)\n",
    "\n",
    "    print(\"T_S.dtype\",T_S.dtype)\n",
    "\n",
    "    print(\"T_S.type\",type(T_S))\n",
    "    \n",
    "    return T_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def normalize_adj(mx):#D^(-1/2)^T*A*D^(-1/2) preprocess 得到一个领域传播位权矩阵\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    normalize_adj process\n",
    "    adj--original matrix shape (2708, 2708)\n",
    "    adj--degree sum matrix shape (2708, 1)\n",
    "    adj--degree inv matrix shape (2708,)\n",
    "    adj--degree no inf matrix shape (2708,)\n",
    "    adj--degree norm diag matrix shape (2708, 2708)\n",
    "    adj--res shape (2708, 2708)\n",
    "    \"\"\"\n",
    "    ## print(\"normalize_adj process\")\n",
    "    ## print(\"adj--original matrix shape\",mx.shape)\n",
    "    rowsum = np.array(mx.sum(1)) #每行求和，求得每个节点的出度列矩阵\n",
    "    ## print(\"adj--degree sum matrix shape\",rowsum.shape)\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    ## print(\"adj--degree inv matrix shape\",r_inv_sqrt.shape)\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    ## print(\"adj--degree no inf matrix shape\",r_inv_sqrt.shape)\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    ## print(\"adj--degree norm diag matrix shape\",r_mat_inv_sqrt.shape)\n",
    "    resm = mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo()\n",
    "    ## print(\"adj--res shape\",resm.shape)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo() #D^(-1/2)*A*D^(-1/2)^T\n",
    "\n",
    "def normalize(mx):#D^-(1)*A\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    normalize process\n",
    "    mx--original matrix shape (2708, 1433)\n",
    "    mx--degree sum matrix shape (2708, 1)\n",
    "    mx--degree inv matrix shape (2708,)\n",
    "    mx--degree no inf matrix shape (2708,)\n",
    "    mx--degree norm diag matrix shape (2708, 2708)\n",
    "    mx--res shape (2708, 1433)\n",
    "    \"\"\"  \n",
    "    ## print(\"normalize process\")\n",
    "    ## print(\"mx--original matrix shape\",mx.shape)\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    ## print(\"mx--degree sum matrix shape\",rowsum.shape)\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    ## print(\"mx--degree inv matrix shape\",r_inv.shape)\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    ## print(\"mx--degree no inf matrix shape\",r_inv.shape)\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    ## print(\"mx--degree norm diag matrix shape\",r_mat_inv.shape)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    ## print(\"mx--res shape\",mx.shape)\n",
    "\n",
    "    print(\"NA.shape\",mx.shape)\n",
    "\n",
    "    print(\"NA.dtype\",mx.dtype)\n",
    "\n",
    "    print(\"NA.type\",type(mx))\n",
    "    return mx\n",
    "\n",
    "def laplacian(mx, norm):\n",
    "    \"\"\"Laplacian-normalize sparse matrix\"\"\"\n",
    "    assert (all (len(row) == len(mx) for row in mx)), \"Input should be a square matrix\"\n",
    "\n",
    "    return csgraph.laplacian(adj, normed = norm)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def load_data(path=\"../Data\", dataset=\"cora\"):\n",
    "    \"\"\"\n",
    "    ind.[:dataset].x     => the feature vectors of the training instances (scipy.sparse.csr.csr_matrix)\n",
    "    ind.[:dataset].y     => the one-hot labels of the labeled training instances (numpy.ndarray)\n",
    "    ind.[:dataset].allx  => the feature vectors of both labeled and unlabeled training instances (csr_matrix)\n",
    "    ind.[:dataset].ally  => the labels for instances in ind.dataset_str.allx (numpy.ndarray)\n",
    "    ind.[:dataset].graph => the dict in the format {index: [index of neighbor nodes]} (collections.defaultdict)\n",
    "    ind.[:dataset].tx => the feature vectors of the test instances (scipy.sparse.csr.csr_matrix)\n",
    "    ind.[:dataset].ty => the one-hot labels of the test instances (numpy.ndarray)\n",
    "    ind.[:dataset].test.index => indices of test instances in graph, for the inductive setting\n",
    "    \"\"\"\n",
    "    print(\"\\n[STEP 1]: Upload {} dataset.\".format(dataset))\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        with open(\"{}/ind.{}.{}\".format(path, dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    \n",
    "    ## print(\"x.shape=\",x.shape)\n",
    "    ## print(\"y.shape=\",y.shape)\n",
    "    ## print(\"tx.shape=\",tx.shape)\n",
    "    ## print(\"ty.shape=\",ty.shape)\n",
    "    ## print(\"allx=\",allx.shape)\n",
    "    ## print(\"ally=\",ally.shape)\n",
    "    ## print(\"len(graph)=\",len(graph))\n",
    "    \"\"\"\n",
    "    x.shape= (140, 1433)\n",
    "    y.shape= (140, 7)\n",
    "    tx.shape= (1000, 1433)\n",
    "    ty.shape= (1000, 7)\n",
    "    allx= (1708, 1433)\n",
    "    ally= (1708, 7)\n",
    "    len(graph)= 2708\n",
    "    \"\"\"\n",
    "    \n",
    "    ## print(\"x=\",x)\n",
    "    ## print(\"y=\",y)\n",
    "    ## print(\"tx=\",tx)\n",
    "    ## print(\"ty=\",ty)\n",
    "    ## print(\"allx=\",allx)\n",
    "    ## print(\"ally=\",ally)\n",
    "    ## print(\"graph=\",graph)\n",
    "\n",
    "    test_idx_reorder = parse_index_file(\"{}/ind.{}.test.index\".format(path, dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        #Citeseer dataset contains some isolated nodes in the graph\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "    print(\"| # of nodes : {}\".format(adj.shape[0]))\n",
    "    print(\"| # of edges : {}\".format(adj.sum().sum()/2))\n",
    "\n",
    "    features = normalize(features) #将feature按照度归一化\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0])) #将A按照两个边端点度归一化 PPR方法\n",
    "    #adj = preprocess_HK(adj+ sp.eye(adj.shape[0]),0.5,0.0001)\n",
    "    \n",
    "    ###### \n",
    "    ### AF MATRIX\n",
    "    ######\n",
    "    #####################\n",
    "    ########################## adj = AF_matrix(adj, features, 3)\n",
    "    #####################\n",
    "    ## AF = AF_matrix(adj, features, 3)\n",
    "    \n",
    "    print(\"| # of features : {}\".format(features.shape[1]))\n",
    "    print(\"| # of clases   : {}\".format(ally.shape[1]))\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    sparse_mx = adj.tocoo().astype(np.float32)\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        save_label = np.where(labels)[1]\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "    idx_test = test_idx_range.tolist()\n",
    "\n",
    "    print(\"| # of train set : {}\".format(len(idx_train)))\n",
    "    print(\"| # of val set   : {}\".format(len(idx_val)))\n",
    "    print(\"| # of test set  : {}\".format(len(idx_test)))\n",
    "\n",
    "    idx_train, idx_val, idx_test = list(map(lambda x: torch.LongTensor(x), [idx_train, idx_val, idx_test]))\n",
    "\n",
    "    def missing_elements(L):\n",
    "        start, end = L[0], L[-1]\n",
    "        return sorted(set(range(start, end+1)).difference(L))\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        L = np.sort(idx_test)\n",
    "        missing = missing_elements(L)\n",
    "\n",
    "        for element in missing:\n",
    "            save_label = np.insert(save_label, element, 0)\n",
    "\n",
    "        labels = torch.LongTensor(save_label)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj_torch(torch_mx):#D^(-1/2)^T*A*D^(-1/2) preprocess 得到一个领域传播位权矩阵\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    normalize_adj process\n",
    "    adj--original matrix shape (2708, 2708)\n",
    "    adj--degree sum matrix shape (2708, 1)\n",
    "    adj--degree inv matrix shape (2708,)\n",
    "    adj--degree no inf matrix shape (2708,)\n",
    "    adj--degree norm diag matrix shape (2708, 2708)\n",
    "    adj--res shape (2708, 2708)\n",
    "    \"\"\"\n",
    "    ## print(\"normalize_adj process\")\n",
    "    ## print(\"adj--original matrix shape\",mx.shape)\n",
    "    \"\"\"\n",
    "    rowsum = np.array(torch_mx.sum(1)) #每行求和，求得每个节点的出度列矩阵\n",
    "    ## print(\"adj--degree sum matrix shape\",rowsum.shape)\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    ## print(\"adj--degree inv matrix shape\",r_inv_sqrt.shape)\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    ## print(\"adj--degree no inf matrix shape\",r_inv_sqrt.shape)\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    ## print(\"adj--degree norm diag matrix shape\",r_mat_inv_sqrt.shape)\n",
    "    resm = np.array(torch_mx).dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo()\n",
    "    ## print(\"adj--res shape\",resm.shape)\n",
    "    resm = torch.from_numpy(resm)\n",
    "    \"\"\"\n",
    "    rowsum = torch_mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum,-0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)]=0.\n",
    "    r_mat_inv_sqrt = torch.diag_embed(r_inv_sqrt)\n",
    "    resm = torch.mm(r_mat_inv_sqrt.T,torch_mx)\n",
    "    resm = torch.mm(resm,r_mat_inv_sqrt)\n",
    "    ## resm.requires_grad = False only can change on the leaf node\n",
    "    return resm#mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo() #D^(-1/2)*A*D^(-1/2)^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj_torch_khop(torch_mx,hop_num,alpha):#D^(-1/2)^T*A*D^(-1/2) preprocess 得到一个领域传播位权矩阵\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    normalize_adj process\n",
    "    adj--original matrix shape (2708, 2708)\n",
    "    adj--degree sum matrix shape (2708, 1)\n",
    "    adj--degree inv matrix shape (2708,)\n",
    "    adj--degree no inf matrix shape (2708,)\n",
    "    adj--degree norm diag matrix shape (2708, 2708)\n",
    "    adj--res shape (2708, 2708)\n",
    "    \"\"\"\n",
    "    ## print(\"normalize_adj process\")\n",
    "    ## print(\"adj--original matrix shape\",mx.shape)\n",
    "    \"\"\"\n",
    "    rowsum = np.array(torch_mx.sum(1)) #每行求和，求得每个节点的出度列矩阵\n",
    "    ## print(\"adj--degree sum matrix shape\",rowsum.shape)\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    ## print(\"adj--degree inv matrix shape\",r_inv_sqrt.shape)\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    ## print(\"adj--degree no inf matrix shape\",r_inv_sqrt.shape)\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    ## print(\"adj--degree norm diag matrix shape\",r_mat_inv_sqrt.shape)\n",
    "    resm = np.array(torch_mx).dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo()\n",
    "    ## print(\"adj--res shape\",resm.shape)\n",
    "    resm = torch.from_numpy(resm)\n",
    "    \"\"\"\n",
    "    for i in range(0,hop_num):\n",
    "        rowsum = torch_mx.sum(1)\n",
    "        r_inv_sqrt = torch.pow(rowsum,-0.5).flatten()\n",
    "        r_inv_sqrt[torch.isinf(r_inv_sqrt)]=0.\n",
    "        r_mat_inv_sqrt = torch.diag_embed(r_inv_sqrt)\n",
    "        resm_new = torch.mm(r_mat_inv_sqrt.T,torch_mx)\n",
    "        resm_new = torch.mm(resm_new,r_mat_inv_sqrt)\n",
    "        if(i == 0):\n",
    "            resm = resm_new\n",
    "        else:\n",
    "            resm = resm + resm_new * alpha\n",
    "            alpha *= alpha\n",
    "        torch_mx = torch.mm(torch_mx,torch_mx)\n",
    "    ## resm.requires_grad = False only can change on the leaf node\n",
    "    return resm#mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo() #D^(-1/2)*A*D^(-1/2)^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj_torch_interaction_conservation(adj):\n",
    "    linesum = adj.sum(1)\n",
    "    line_degree = linesum.expand(adj.shape[1],adj.shape[1])\n",
    "    adj = adj/line_degree\n",
    "\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_diffusion_descent_multi_times(torch_mx,hop_num,alpha):#D^(-1/2)^T*A*D^(-1/2) preprocess 得到一个领域传播位权矩阵\n",
    "    \n",
    "    for i in range(0,hop_num):\n",
    "        normalize_adj_torch_interaction_conservation(torch_mx)\n",
    "        # i-->hop_num\n",
    "        # resm_new(i,j)*alpha means the similarity betweent the i and j, it can be view as the 1./v\n",
    "        # as for the diffusion formula, that is the f = C*exp(-d*v) the larger distance and larger speed \n",
    "        # can result in a great diffusion factor while the little distance with small speed can result in \n",
    "        # a slow speed.\n",
    "        # Here another is the transferring time, here we consider a small domain quick transferring scenery\n",
    "        # where the transferring speed is constant and the interaction happens quickly where\n",
    "        # the temperature will have no change, the asynchronous update will show as multiple time quick update\n",
    "        rowsum = torch_mx.sum(1)\n",
    "        r_inv_sqrt = torch.pow(rowsum,-0.5).flatten()\n",
    "        r_inv_sqrt[torch.isinf(r_inv_sqrt)]=0.\n",
    "        r_mat_inv_sqrt = torch.diag_embed(r_inv_sqrt)\n",
    "        resm_new = torch.mm(r_mat_inv_sqrt.T,torch_mx)\n",
    "        resm_new = torch.mm(resm_new,r_mat_inv_sqrt) # here resm_new represents the v matrix, larger v, more correlated\n",
    "        ### here means the resm_new as v\n",
    "        ### factor_m = torch.exp(-(i+1)*resm_new) # here factor_m represents the diffusion decreases matri\n",
    "        ### factor_m[factor_m==1] = 0 #here must be the similarity be 0\n",
    "        \n",
    "        # Descent_Factor = C*exp(-d^2/t) = C*exp(-d*v) \n",
    "         \n",
    "        \n",
    "        # hop_num\n",
    "\n",
    "        ### here means the resm_new as 1/v\n",
    "        expmm = (i+1)/resm_new\n",
    "        expmm[torch.isinf(expmm)] = 0\n",
    "        factor_m = torch.exp(-expmm)\n",
    "        # print(\"prefactorsm.shape\",factor_m.shape)\n",
    "        # print(\"prefactorm\",factor_m)\n",
    "\n",
    "        ### here means the resm_new as v\n",
    "        ### times_m = (i+1)/resm_new # here the d./v matix shows the transferring times matrix\n",
    "        ### times_m[torch.isinf(times_m)]=0\n",
    "\n",
    "        ### here means the resm_new as 1/v\n",
    "        times_m = (i+1)*resm_new\n",
    "        # print(\"times_m.shape\",times_m.shape)\n",
    "        # print(\"times_m\",times_m)\n",
    "\n",
    "        factors_m = factor_m * times_m\n",
    "        # print(\"factors_m.shape\",factor_m.shape)\n",
    "        # print(\"factors_m\",factor_m)\n",
    "        if(i == 0):\n",
    "            resm = factors_m\n",
    "        else:\n",
    "            resm = resm + factors_m*alpha\n",
    "            alpha *= alpha\n",
    "        # print(\"resm\",resm)\n",
    "        \n",
    "        torch_mx = torch.mm(torch_mx,torch_mx)\n",
    "    ## resm.requires_grad = False only can change on the leaf node\n",
    "    return resm#mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo() #D^(-1/2)*A*D^(-1/2)^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  t: 5.0\\n  k: 128\\n  eps: 0.0001\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_k_matrix(A, k: int = 128):\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = torch.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A, eps: float = 0.01):\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix,\n",
    "        t: float = 5.0):\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + torch.eye(num_nodes)\n",
    "    D_tilde = torch.diag(1/torch.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return torch.exp(-t * (torch.eye(num_nodes) - H))\n",
    "\n",
    "\"\"\"\n",
    "  t: 5.0\n",
    "  k: 128\n",
    "  eps: 0.0001\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AF_matrix(adj, feature, hop_num):#D^(-1/2)^T*(AF)*(AF)^T*D^(-1/2)\n",
    "    #L_AF self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "    \n",
    "    #PVM\n",
    "    \n",
    "    # adj 2708, feature 1433\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    For adjancy matrix\n",
    "    normalize_adj process\n",
    "    adj--original matrix shape (2708, 2708)\n",
    "    adj--degree sum matrix shape (2708, 1)\n",
    "    adj--degree inv matrix shape (2708,)\n",
    "    adj--degree no inf matrix shape (2708,)\n",
    "    adj--degree norm diag matrix shape (2708, 2708)\n",
    "    adj--res shape (2708, 2708)\n",
    "    \n",
    "    For feature matrix\n",
    "    normalize process\n",
    "    mx--original matrix shape (2708, 1433)\n",
    "    mx--degree sum matrix shape (2708, 1)\n",
    "    mx--degree inv matrix shape (2708,)\n",
    "    mx--degree no inf matrix shape (2708,)\n",
    "    mx--degree norm diag matrix shape (2708, 2708)\n",
    "    mx--res shape (2708, 1433)\n",
    "    \"\"\"\n",
    "    A_curhop = adj\n",
    "    resm = adj\n",
    "    dvec = torch.ones(adj.shape[0],1)#torch.ones(3*hop_num,1)\n",
    "    L_AF = Parameter(torch.FloatTensor(1,3*hop_num))\n",
    "    torch.nn.init.normal_(L_AF)\n",
    "    \"\"\"\n",
    "    1 0 1\n",
    "    1 0 1\n",
    "    1 0 1\n",
    "    对应一个对应AF矩阵的值\n",
    "    \"\"\"\n",
    "    # PVM = np.array(torch.FloatTensor(3*hop_num,(adj.shape[1]+adj.shape[1]+feature.shape[1])*hop_num))\n",
    "    PVM = torch.ones(3*hop_num,adj.shape[1])*L_AF[0][0]\n",
    "    print(\"PVM.shape\",PVM.shape)\n",
    "    for i in range(1,3*hop_num):\n",
    "        print(\"i'th PVM expansion\")\n",
    "        if((i%3) == 2) :\n",
    "            PVM = torch.cat((PVM,torch.ones(3*hop_num,feature.shape[1])*L_AF[0][i]),1)\n",
    "            print(PVM.shape)\n",
    "        else:\n",
    "            PVM = torch.cat((PVM,torch.ones(3*hop_num,adj.shape[1])*L_AF[0][i]),1)\n",
    "            print(PVM.shape)\n",
    "    \"\"\"\n",
    "    PVM = None\n",
    "    for i in range(0,3*hop_num):\n",
    "        if(i%3 == 0):\n",
    "            PVM.concat((PVM,np.ones(3*hop_num,adj.shape[1])*L_AF[i]))\n",
    "        else:\n",
    "            PVM.concat((PVM,np.ones(3*hop_num,feature.shape[1])*L_AF[i]))\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0,hop_num):\n",
    "        print(\"{}'th hop normalize_adj process\".format(i))\n",
    "        A_curhop_D = np.array(A_curhop.sum(1)).flatten() #每行求和，求得每个节点的出度列矩阵 A_curhop_D var mean\n",
    "        ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)\n",
    "        A_curhop_D = sp.diags(A_curhop_D) #var mean\n",
    "        ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)\n",
    "        # 进行A^k的k步传播，选择Feature矩阵的非0位置\n",
    "        feature_curhop = A_curhop.dot(feature)#feature[A_cur_hop where not 0] #var mean\n",
    "        ##print(\"feature_curhop.shape\",feature_curhop.shape)\n",
    "        #degree_hop = (A_cur_hop[A_cur_hop!=0]=A_curhop_D) #var mean\n",
    "        \"\"\"\n",
    "        print(\"the adj matrix\",A_curhop)\n",
    "        print(\"the adj matrix.shape\",A_curhop.shape)\n",
    "        print(\"the degree matrix\",A_curhop_D)\n",
    "        print(\"the degree matrix.shape\",A_curhop_D.shape)\n",
    "        print(\"the adj feature matrix\",feature_curhop) #A^hopnum * feature\n",
    "        print(\"the adj feature matrix.shape\",feature_curhop.shape)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        adj_rowsum = np.array(A_cur_hop.sum(1)) #每行求和，求得每个节点的出度列矩阵 A_curhop_D\n",
    "        print(\"adj--degree sum matrix shape\",adj_rowsum.shape)\n",
    "        adj_r_inv_sqrt = np.power(adj_rowsum, -0.5).flatten()\n",
    "        print(\"adj--degree inv matrix shape\",adj_r_inv_sqrt.shape)#A_curhop_D^(-1/2)\n",
    "        adj_r_inv_sqrt[np.isinf(adj_r_inv_sqrt)] = 0.\n",
    "        print(\"adj--degree no inf matrix shape\",adj_r_inv_sqrt.shape)\n",
    "        adj_r_mat_inv_sqrt = sp.diags(adj_r_inv_sqrt) #A_D^(-1/2)\n",
    "        print(\"adj--degree norm diag matrix shape\",adj_r_mat_inv_sqrt.shape) #A_curhop_D^(-1/2)\n",
    "        #norm_adj_resm = adj.dot(adj_r_mat_inv_sqrt).transpose().dot(adj_r_mat_inv_sqrt).tocoo()\n",
    "        #print(\"adj--res shape\",resm.shape)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(\"concatenate members shape\")\n",
    "        print(\"A_curhop.shape=\",A_curhop.shape)\n",
    "        print(\"A_curhop_D.shape\",A_curhop_D.shape)\n",
    "        \"\"\"\n",
    "        if(i !=0):\n",
    "            resm = sp.hstack((A_curhop, resm))\n",
    "        ##print(\"first time concat.shape\",resm.shape)\n",
    "        ##print(\"feature_curhop.shape\",feature_curhop.shape)\n",
    "        resm = sp.hstack((A_curhop_D, resm))\n",
    "        resm = sp.hstack((feature_curhop, resm))\n",
    "        ##print(\"the resm here is\")\n",
    "        ##print(resm.shape)\n",
    "        A_curhop  =A_curhop.dot(adj)\n",
    "    print(\"dvec shape\")\n",
    "    print(dvec.shape) #(9, 1)\n",
    "    print(\"L_AF shape\")\n",
    "    print(L_AF.shape) # torch.Size([1, 9])\n",
    "    print(\"PVM shape\")\n",
    "    print(PVM.shape) # (9, 20547) 1|0|0|1|1|0\n",
    "    # (9,1)*(1,9)*(9,20547)\n",
    "    \"\"\"\n",
    "    L_AF shape\n",
    "    torch.Size([1, 9])\n",
    "    PVM shape\n",
    "    (9, 20547)\n",
    "    | # of features : 1433\n",
    "    | # of clases   : 7\n",
    "    | # of train set : 140\n",
    "    | # of val set   : 500\n",
    "    | # of test set  : 1000\n",
    "    \"\"\"\n",
    "    \n",
    "    #resm #= sp.coo_matrix(a_matrix)#torch.from_numpy(resm) #tmp_coo=sp.coo_matrix(a_matrix)\n",
    "    resm = resm\n",
    "    values=resm.data\n",
    "    indices=np.vstack((resm.row,resm.col))\n",
    "    i=torch.LongTensor(indices)\n",
    "    v=torch.LongTensor(values)\n",
    "    resm =torch.sparse_coo_tensor(i,v,resm.shape)\n",
    "    resm = resm.to_dense()\n",
    "    \n",
    "    mux_res = torch.mm(torch.mm(dvec,L_AF),PVM) #resm*L_AF*PVM\n",
    "    print(\"mux_res result size\",mux_res.shape)\n",
    "    #print(\"preprocess matrix\",mux_res)\n",
    "    \n",
    "    print(\"the actual A-F matrix shape\",resm.shape)\n",
    "    \n",
    "    weighted_resm = torch.mul(mux_res,resm)#mux_res * resm\n",
    "    \n",
    "    mm_res = torch.mm(weighted_resm,weighted_resm.T)\n",
    "    print(\"result multiplication shape\",mm_res.shape)\n",
    "    \n",
    "    return mm_res #resm,L_AF,PVM #D^(-1/2)*A*D^(-1/2)^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model,record):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj, features)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward(retain_graph=True)  \n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    output = model(features, adj, features)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'acc_test: {:.4f}'.format(acc_test.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    record[acc_val.item()] = acc_test.item() #record: acc_val->acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, hop_num,bias=True):\n",
    "        ## print(\"init_state of GCN\")\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        ######此处定义了网路的parameter例如weight和bias 两者在init里面定义为parameter 在self保存为net的结构\n",
    "        self.hop_num = hop_num\n",
    "        self.L_AF = Parameter(torch.FloatTensor(1,3*hop_num))\n",
    "        self.L_AF2 = Parameter(torch.FloatTensor(1,2*hop_num))\n",
    "        factor = 1\n",
    "        \"\"\"\n",
    "        for i in range(0,hop_num):\n",
    "            self.L_AF[3*i] = factor\n",
    "            self.L_AF[3*i+1] = factor*0.1\n",
    "            self.L_AF[3*i+1] = factor*0.001\n",
    "            factor *=0.1\n",
    "        \"\"\"\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features)) #线性Linear层负责convolutional变换 \n",
    "        #是两个维度的转换 为1433,16 16.7\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features)) #bias大小为输出层上的偏执 可以为16 7\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        ## print(\"reset_state of GCN\")\n",
    "        ### reset linear layer\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        ## print(\"stdv=\",stdv)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj, feature):\n",
    "        ###adj = self.AF_matrix(adj, feature, self.hop_num) #k-hop learnable AF hop\n",
    "        #adj = normalize_adj_torch(adj) #1-hop normalization\n",
    "        #adj = normalize_adj_torch_khop(adj,self.hop_num,0.1) #k-hop A prop with k-degree normalization\n",
    "\n",
    "        # structure imbalance\n",
    "        \n",
    "        # AF-ture\n",
    "        # structure sampling -> local sampling (walk) * global parameter\n",
    "\n",
    "        # heat interaction in local domain --> global insight\n",
    "\n",
    "\n",
    "        adj = self.AK_matrix(adj,self.hop_num) #k-hop A prop with learnable params\n",
    "        \"\"\"\n",
    "        adj = get_heat_matrix(adj,t=5.0)\n",
    "        adj = get_top_k_matrix(adj, k=128)\n",
    "        adj = get_clipped_matrix(adj, eps=0.0001)\n",
    "        \"\"\"\n",
    "        \n",
    "        ##adj = normalize_diffusion_descent_multi_times(adj,3,0.1)\n",
    "        adj = normalize_adj_torch_interaction_conservation(adj)\n",
    "        \"\"\"\n",
    "        print(\"forward_state of GCN\")\n",
    "        print(\"input.shape=\",input.shape) #input.shape= torch.Size([2708, 1433]) 0/1\n",
    "        print(\"input=\",input)\n",
    "        print(\"adj.shape=\",adj.shape) #adj.shape= torch.Size([2708, 2708]) float\n",
    "        print(\"adj=\",adj)\n",
    "        \"\"\"\n",
    "        support = torch.mm(input, self.weight) #torch.mm矩阵乘法 线性层的个数为 1433*16 (in_features, out_features)\n",
    "        \"\"\"\n",
    "        print(\"support.shape=\",support.shape) #support.shape= torch.Size([2708, 16]) float\n",
    "        print(\"support=\",support)\n",
    "        \"\"\"\n",
    "        output = torch.spmm(adj, support) #A*F*L spmm稀疏矩阵乘法\n",
    "        \"\"\"\n",
    "        print(\"output.shape=\",output.shape)#output.shape= torch.Size([2708, 16]) float\n",
    "        print(\"output=\",output)\n",
    "        print(\"bias.shape=\",self.bias.shape) #bias.shape= torch.Size([16]) float\n",
    "        print(\"bias=\",self.bias)\n",
    "        \"\"\"\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        print(\"repr_state of GCN\")\n",
    "        print(self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')')\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    def AFtune_matrix(self, adj, feature, hop_num):#D^(-1/2)^T*(AF)*(AF)^T*D^(-1/2) --somethinking feature-oriented tune and label-oriented tune\n",
    "        # label\n",
    "        # dense Feature\n",
    "        adj = normalize_adj_torch(adj)\n",
    "        A_curhop = adj\n",
    "        resm = adj\n",
    "        dvec = torch.ones(adj.shape[0],1)#torch.ones(3*hop_num,1)\n",
    "        #L_AF = Parameter(torch.FloatTensor(1,3*hop_num))\n",
    "        torch.nn.init.normal_(self.L_AF)\n",
    "        # PVM = np.array(torch.FloatTensor(3*hop_num,(adj.shape[1]+adj.shape[1]+feature.shape[1])*hop_num))\n",
    "        PVM = torch.ones(3*hop_num,adj.shape[1])*self.L_AF[0][0]\n",
    "        factor = 1.0\n",
    "        for i in range(1,3*hop_num):\n",
    "            factor *= 0.1\n",
    "            ## print(\"i'th PVM expansion\")\n",
    "            if((i%3) == 2) :\n",
    "                PVM = torch.cat((PVM,torch.ones(3*hop_num,feature.shape[1])*self.L_AF[0][i]),1)\n",
    "                ## print(PVM.shape)\n",
    "            else:\n",
    "                PVM = torch.cat((PVM,torch.ones(3*hop_num,adj.shape[1])*self.L_AF[0][i]),1)\n",
    "                ## print(PVM.shape)\n",
    "\n",
    "        for i in range(0,hop_num):\n",
    "            ## print(\"{}'th hop normalize_adj process\".format(i))\n",
    "            A_curhop_D = A_curhop.sum(1).flatten() #每行求和，求得每个节点的出度列矩阵 A_curhop_D var mean\n",
    "            ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)\n",
    "            A_curhop_D = A_curhop_D * torch.eye(A_curhop_D.shape[0]) #var mean\n",
    "            ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)torch.eye(A_curhop_D.shape[1])\n",
    "            # 进行A^k的k步传播，选择Feature矩阵的非0位置\n",
    "            feature_curhop = torch.mm(A_curhop,feature)\n",
    "            if(i !=0):\n",
    "                resm = torch.cat((A_curhop,resm), 1)\n",
    "            resm = torch.cat((A_curhop_D, resm),1)\n",
    "            #resm = sp.hstack((feature_curhop, resm))\n",
    "            resm = torch.cat((feature_curhop, resm),1)\n",
    "            A_curhop = torch.mm(A_curhop,adj)\n",
    "            A_curhop = normalize_adj_torch(A_curhop)\n",
    "\n",
    "        mux_res = torch.mm(torch.mm(dvec,self.L_AF),PVM) #resm*L_AF*PVM\n",
    "\n",
    "        weighted_resm = torch.mul(mux_res,resm)#mux_res * resm\n",
    "\n",
    "        mm_res = torch.mm(weighted_resm,weighted_resm.T)\n",
    "        ## print(\"result multiplication shape\",mm_res.shape)\n",
    "        # mm_res.requires_grad=False\n",
    "\n",
    "        return mm_res #resm,L_AF,PVM #D^(-1/2)*A*D^(-1/2)^T\n",
    "    \n",
    "    def AF_matrix(self, adj, feature, hop_num):#D^(-1/2)^T*(AF)*(AF)^T*D^(-1/2)\n",
    "        #L_AF self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "        #PVM\n",
    "\n",
    "        # adj 2708, feature 1433\n",
    "        adj = normalize_adj_torch(adj)\n",
    "        A_curhop = adj\n",
    "        resm = adj\n",
    "        dvec = torch.ones(adj.shape[0],1)#torch.ones(3*hop_num,1)\n",
    "        #L_AF = Parameter(torch.FloatTensor(1,3*hop_num))\n",
    "        torch.nn.init.normal_(self.L_AF)\n",
    "        # PVM = np.array(torch.FloatTensor(3*hop_num,(adj.shape[1]+adj.shape[1]+feature.shape[1])*hop_num))\n",
    "        PVM = torch.ones(3*hop_num,adj.shape[1])*self.L_AF[0][0]\n",
    "        factor = 1.0\n",
    "        ## print(\"PVM.shape\",PVM.shape)\n",
    "        for i in range(1,3*hop_num):\n",
    "            factor *= 0.1\n",
    "            ## print(\"i'th PVM expansion\") A*F (A^n)*F\n",
    "            if((i%3) == 2) :\n",
    "                PVM = torch.cat((PVM,torch.ones(3*hop_num,feature.shape[1])*self.L_AF[0][i]*factor*0.00001),1)\n",
    "                ## print(PVM.shape)\n",
    "            else:\n",
    "                PVM = torch.cat((PVM,torch.ones(3*hop_num,adj.shape[1])*self.L_AF[0][i]*factor),1)\n",
    "                ## print(PVM.shape)\n",
    "\n",
    "        for i in range(0,hop_num):\n",
    "            ## print(\"{}'th hop normalize_adj process\".format(i))\n",
    "            A_curhop_D = A_curhop.sum(1).flatten() #每行求和，求得每个节点的出度列矩阵 A_curhop_D var mean\n",
    "            ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)\n",
    "            A_curhop_D = A_curhop_D * torch.eye(A_curhop_D.shape[0]) #var mean\n",
    "            ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)torch.eye(A_curhop_D.shape[1])\n",
    "            # 进行A^k的k步传播，选择Feature矩阵的非0位置\n",
    "            feature_curhop = torch.mm(A_curhop,feature)\n",
    "            if(i !=0):\n",
    "                resm = torch.cat((A_curhop,resm), 1)\n",
    "            ##print(\"first time concat.shape\",resm.shape)\n",
    "            ##print(\"feature_curhop.shape\",feature_curhop.shape)\n",
    "            #resm = sp.hstack((A_curhop_D, resm))\n",
    "            resm = torch.cat((A_curhop_D, resm),1)\n",
    "            #resm = sp.hstack((feature_curhop, resm))\n",
    "            resm = torch.cat((feature_curhop, resm),1)\n",
    "            A_curhop = torch.mm(A_curhop,adj)\n",
    "            #A_curhop = normalize_adj_torch(A_curhop)->one-hop normalization multi-hop normalization\n",
    "\n",
    "        mux_res = torch.mm(torch.mm(dvec,self.L_AF),PVM) #resm*L_AF*PVM\n",
    "\n",
    "        weighted_resm = torch.mul(mux_res,resm)#mux_res * resm\n",
    "\n",
    "        mm_res = torch.mm(weighted_resm,weighted_resm.T)\n",
    "        ## print(\"result multiplication shape\",mm_res.shape)\n",
    "        # mm_res.requires_grad=False\n",
    "\n",
    "        return mm_res #resm,L_AF,PVM #D^(-1/2)*A*D^(-1/2)^T\n",
    "\n",
    "    def AK_matrix(self, adj, hop_num):#D^(-1/2)^T*(AF)*(AF)^T*D^(-1/2)\n",
    "        #L_AF self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "        #PVM\n",
    "\n",
    "        # adj 2708, feature 1433\n",
    "        adj = normalize_adj_torch(adj)\n",
    "        A_curhop = adj\n",
    "        resm = adj\n",
    "        dvec = torch.ones(adj.shape[0],1)#torch.ones(3*hop_num,1)\n",
    "        #L_AF = Parameter(torch.FloatTensor(1,3*hop_num))\n",
    "        torch.nn.init.normal_(self.L_AF2)\n",
    "        # PVM = np.array(torch.FloatTensor(3*hop_num,(adj.shape[1]+adj.shape[1]+feature.shape[1])*hop_num))\n",
    "        PVM = torch.ones(2*hop_num,adj.shape[1])*self.L_AF2[0][0]\n",
    "        factor = 1.0\n",
    "        ## print(\"PVM.shape\",PVM.shape)\n",
    "        for i in range(1,2*hop_num):\n",
    "            factor *= 0.1\n",
    "            ## print(\"i'th PVM expansion\") A*F (A^n)*F\n",
    "            PVM = torch.cat((PVM,torch.ones(2*hop_num,adj.shape[1])*self.L_AF2[0][i]*factor),1)\n",
    "            ## print(PVM.shape)\n",
    "\n",
    "        for i in range(0,hop_num):\n",
    "            ## print(\"{}'th hop normalize_adj process\".format(i))\n",
    "            A_curhop_D = A_curhop.sum(1).flatten() #每行求和，求得每个节点的出度列矩阵 A_curhop_D var mean\n",
    "            ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)\n",
    "            A_curhop_D = A_curhop_D * torch.eye(A_curhop_D.shape[0]) #var mean\n",
    "            ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)torch.eye(A_curhop_D.shape[1])\n",
    "            # 进行A^k的k步传播，选择Feature矩阵的非0位置\n",
    "            # feature_curhop = torch.mm(A_curhop,feature)\n",
    "            if(i !=0):\n",
    "                resm = torch.cat((A_curhop,resm), 1)\n",
    "            ##print(\"first time concat.shape\",resm.shape)\n",
    "            ##print(\"feature_curhop.shape\",feature_curhop.shape)\n",
    "            #resm = sp.hstack((A_curhop_D, resm))\n",
    "            resm = torch.cat((A_curhop_D, resm),1)\n",
    "            #resm = sp.hstack((feature_curhop, resm))\n",
    "            # resm = torch.cat((feature_curhop, resm),1)\n",
    "            A_curhop = torch.mm(A_curhop,adj)\n",
    "            #A_curhop = normalize_adj_torch(A_curhop)->one-hop normalization multi-hop normalization\n",
    "\n",
    "        mux_res = torch.mm(torch.mm(dvec,self.L_AF2),PVM) #resm*L_AF*PVM\n",
    "\n",
    "        weighted_resm = torch.mul(mux_res,resm)#mux_res * resm\n",
    "\n",
    "        mm_res = torch.mm(weighted_resm,weighted_resm.T)\n",
    "        ## print(\"result multiplication shape\",mm_res.shape)\n",
    "        # mm_res.requires_grad=False\n",
    "\n",
    "        return mm_res #resm,L_AF,PVM #D^(-1/2)*A*D^(-1/2)^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1]: Upload cora dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_641034/2773929969.py:94: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| # of nodes : 2708\n",
      "| # of edges : 5278.0\n",
      "NA.shape (2708, 1433)\n",
      "NA.dtype float32\n",
      "NA.type <class 'scipy.sparse._csr.csr_matrix'>\n",
      "| # of features : 1433\n",
      "| # of clases   : 7\n",
      "| # of train set : 140\n",
      "| # of val set   : 500\n",
      "| # of test set  : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_641034/2773929969.py:142: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_matrix = get_heat_matrix(adj,t=5.0)\n",
    "heat_matrix = get_top_k_matrix(heat_matrix, k=128)\n",
    "heat_matrix = get_clipped_matrix(heat_matrix, eps=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正常2层的GCN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout,hop_num):\n",
    "        ## print(\"init_state of 2-layer GCN\")\n",
    "    \n",
    "        super(GCN1, self).__init__() #inherent a GCN model\n",
    "        \n",
    "        \n",
    "        #两层graph convolution\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid,bias=True,hop_num=3)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass,bias=True,hop_num=3)\n",
    "        self.dropout = dropout\n",
    "        self.hop_num = hop_num\n",
    "        \n",
    "        ## print(\"gc1=\",self.gc1)\n",
    "        ## print(\"gc2=\",self.gc2)\n",
    "        ## print(\"dropout=\",self.dropout) #0.5\n",
    "\n",
    "    def forward(self, x, adj,features):\n",
    "        ## print(\"forward_state of 2-layer GCN\")\n",
    "        x_d = F.dropout(x, self.dropout, training=self.training) #对于feature进行drop_out \n",
    "        \"\"\"\n",
    "        print(\"x_d.shape=\",x_d.shape) #x_d.shape= torch.Size([2708, 1433])\n",
    "        print(\"x_d=\",x_d)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        x_d= tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        ...,\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
    "        \"\"\"\n",
    "        ### layer 1 processed result\n",
    "        x = self.gc1(x_d, adj, features) \n",
    "        #传入的参数有上一轮的一阶邻居聚合信息的映射 x_d, adj \n",
    "        #for new network here should be changed to AF^T*AF\n",
    "        \"\"\"\n",
    "        print(\"layer1 res.shape\",lrs1.shape) #layer1 res.shape torch.Size([2708, 16])\n",
    "        print(\"layer1 res\",lrs1) # AXW^0 #每次提取周围邻居的信息 A为边信息两边度的权重积 并aggregate到自己的信息来 对于1433维度信息做一个统一的线性层映射\n",
    "        \"\"\"\n",
    "        ########################\n",
    "        #我们现在的工作就是要把AX这个仅仅考虑结构的propagte加一个考虑feature的Propagate，\n",
    "        #还有加一个异步的propagate，还有加一个考虑时间和距离衰减的propagate，同时在一定时间后给一定范围内的节点进行Normalization\n",
    "        ########################\n",
    "        x = F.relu(x)#self.gc1(x_d, adj, features)) ################## i-j 已用feature计算\n",
    "        # X=relu(AXW^0) 这里考虑AXW^0 A[node_num,node_num]*X[node_num,feature_num]*W[feature_num,convolved_dim]\n",
    "        # X=[node_num,pre_convolved_dim]\n",
    "        # AXW^i=A[node_num,node_num]*X[node_num,pre_convolved_dim]*L[pre_convolved_dim,latter_convolved_dim]\n",
    "        #A-F = [node_num,LongAF]\n",
    "        # Org.A = (i,j)1-hopAttentioned-A\n",
    "        # A-F^T*A-F = (i,j)multi-hopAttentioned-A\n",
    "        \"\"\"\n",
    "        print(\"after relu GCN1 x.shape=\",x.shape) #after relu GCN1 x.shape= torch.Size([2708, 16])\n",
    "        print(\"after relu GCN1 x=\",x)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        after relu GCN1 x= tensor([[0.0000, 0.0000, 0.1960,  ..., 0.1392, 0.0000, 0.0000],\n",
    "        [0.0256, 0.0000, 0.1576,  ..., 0.2085, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.1568,  ..., 0.1526, 0.0000, 0.0000],\n",
    "        ...,\n",
    "        [0.0000, 0.0000, 0.0998,  ..., 0.1981, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.1614,  ..., 0.1271, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.1632,  ..., 0.1248, 0.0000, 0.0000]],\n",
    "       grad_fn=<ReluBackward0>)\n",
    "        \"\"\"\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \"\"\"\n",
    "        print(\"after dropout x.shape=\",x.shape)\n",
    "        print(\"after dropout x=\",x)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        after dropout x= tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.3152,  ..., 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.3136,  ..., 0.3053, 0.0000, 0.0000],\n",
    "        ...,\n",
    "        [0.0000, 0.0000, 0.1997,  ..., 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.3229,  ..., 0.2542, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.3264,  ..., 0.2497, 0.0000, 0.0000]],\n",
    "       grad_fn=<MulBackward0>)\n",
    "        \"\"\"\n",
    "        x = self.gc2(x, adj, features) #16->7 AXW^1\n",
    "        \"\"\"\n",
    "        print(\"GCN2 x.shape=\",x.shape) #GCN2 x.shape= torch.Size([2708, 7])\n",
    "        print(\"GCN2 x=\",x)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        GCN2 x= tensor([[-0.1629,  0.0549,  0.0964,  ...,  0.2143,  0.2979,  0.0673],\n",
    "        [-0.1236,  0.0818,  0.0692,  ...,  0.2229,  0.3301,  0.0718],\n",
    "        [-0.1413,  0.0453,  0.0838,  ...,  0.2303,  0.3301,  0.0501],\n",
    "        ...,\n",
    "        [-0.1111,  0.0988,  0.1083,  ...,  0.2313,  0.3120,  0.1059],\n",
    "        [-0.0725,  0.1234,  0.0693,  ...,  0.2215,  0.3570,  0.1172],\n",
    "        [-0.0930,  0.1279,  0.0991,  ...,  0.2040,  0.3516,  0.1457]],\n",
    "       grad_fn=<AddBackward0>)\n",
    "        \"\"\"\n",
    "        res = F.log_softmax(x, dim=1) #对于结果进行归一化 softmax(AXW^1)\n",
    "        \"\"\"\n",
    "        print(\"res.shape=\",res.shape) #res.shape= torch.Size([2708, 7])\n",
    "        print(\"res=\",res)\n",
    "        \"\"\"\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1433, 16])\n",
      "torch.Size([16])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([16, 7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5,hop_num=3) \n",
    "for item in model1.parameters():\n",
    "        print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters setting here\n",
      "features.shape torch.Size([2708, 1433])\n",
      "labels.max().item() 6\n",
      "Epoch: 0001 loss_train: 1.9481 acc_train: 0.1357 acc_val: 0.1640 acc_test: 0.1460 time: 1.9905s\n",
      "Epoch: 0002 loss_train: 1.9384 acc_train: 0.1929 acc_val: 0.1640 acc_test: 0.1470 time: 1.9185s\n",
      "Epoch: 0003 loss_train: 1.9444 acc_train: 0.1643 acc_val: 0.1640 acc_test: 0.1490 time: 1.8612s\n",
      "Epoch: 0004 loss_train: 1.9485 acc_train: 0.1500 acc_val: 0.1620 acc_test: 0.1510 time: 1.8667s\n",
      "Epoch: 0005 loss_train: 1.9389 acc_train: 0.1286 acc_val: 0.1680 acc_test: 0.1630 time: 1.8883s\n",
      "Epoch: 0006 loss_train: 1.9327 acc_train: 0.1857 acc_val: 0.1820 acc_test: 0.2060 time: 1.9096s\n",
      "Epoch: 0007 loss_train: 1.9331 acc_train: 0.1643 acc_val: 0.2260 acc_test: 0.2590 time: 1.8788s\n",
      "Epoch: 0008 loss_train: 1.9218 acc_train: 0.1714 acc_val: 0.2960 acc_test: 0.3040 time: 1.9336s\n",
      "Epoch: 0009 loss_train: 1.9302 acc_train: 0.2071 acc_val: 0.3240 acc_test: 0.3400 time: 1.8678s\n",
      "Epoch: 0010 loss_train: 1.8853 acc_train: 0.3357 acc_val: 0.3180 acc_test: 0.3250 time: 1.8468s\n",
      "Epoch: 0011 loss_train: 1.9211 acc_train: 0.2929 acc_val: 0.3080 acc_test: 0.2880 time: 1.9271s\n",
      "Epoch: 0012 loss_train: 1.8902 acc_train: 0.2643 acc_val: 0.2980 acc_test: 0.2700 time: 1.9901s\n",
      "Epoch: 0013 loss_train: 1.8822 acc_train: 0.2643 acc_val: 0.2940 acc_test: 0.2630 time: 1.9828s\n",
      "Epoch: 0014 loss_train: 1.8936 acc_train: 0.2571 acc_val: 0.2420 acc_test: 0.2210 time: 2.0387s\n",
      "Epoch: 0015 loss_train: 1.8676 acc_train: 0.3071 acc_val: 0.3120 acc_test: 0.2820 time: 2.0733s\n",
      "Epoch: 0016 loss_train: 1.8937 acc_train: 0.4000 acc_val: 0.3300 acc_test: 0.2960 time: 2.0119s\n",
      "Epoch: 0017 loss_train: 1.8345 acc_train: 0.4000 acc_val: 0.3440 acc_test: 0.3190 time: 1.9711s\n",
      "Epoch: 0018 loss_train: 1.8593 acc_train: 0.3429 acc_val: 0.3560 acc_test: 0.3300 time: 2.0022s\n",
      "Epoch: 0019 loss_train: 1.8465 acc_train: 0.3929 acc_val: 0.3740 acc_test: 0.3670 time: 2.0300s\n",
      "Epoch: 0020 loss_train: 1.8313 acc_train: 0.3929 acc_val: 0.4020 acc_test: 0.4010 time: 1.9835s\n",
      "Epoch: 0021 loss_train: 1.8350 acc_train: 0.3643 acc_val: 0.4000 acc_test: 0.4120 time: 1.9615s\n",
      "Epoch: 0022 loss_train: 1.8188 acc_train: 0.3643 acc_val: 0.4220 acc_test: 0.4460 time: 2.0561s\n",
      "Epoch: 0023 loss_train: 1.8330 acc_train: 0.4357 acc_val: 0.4620 acc_test: 0.4750 time: 1.9950s\n",
      "Epoch: 0024 loss_train: 1.7930 acc_train: 0.4786 acc_val: 0.4620 acc_test: 0.4870 time: 2.0110s\n",
      "Epoch: 0025 loss_train: 1.7799 acc_train: 0.4571 acc_val: 0.5620 acc_test: 0.5820 time: 2.0267s\n",
      "Epoch: 0026 loss_train: 1.8100 acc_train: 0.3786 acc_val: 0.4960 acc_test: 0.5260 time: 1.9546s\n",
      "Epoch: 0027 loss_train: 1.7763 acc_train: 0.4071 acc_val: 0.6220 acc_test: 0.6480 time: 2.0229s\n",
      "Epoch: 0028 loss_train: 1.8388 acc_train: 0.3214 acc_val: 0.7100 acc_test: 0.7350 time: 1.9798s\n",
      "Epoch: 0029 loss_train: 1.7188 acc_train: 0.5214 acc_val: 0.5160 acc_test: 0.5410 time: 1.9579s\n",
      "Epoch: 0030 loss_train: 1.7237 acc_train: 0.4929 acc_val: 0.5460 acc_test: 0.5510 time: 2.0250s\n",
      "Epoch: 0031 loss_train: 1.7472 acc_train: 0.4929 acc_val: 0.6000 acc_test: 0.6090 time: 2.0262s\n",
      "Epoch: 0032 loss_train: 1.6998 acc_train: 0.4857 acc_val: 0.5440 acc_test: 0.5360 time: 1.9752s\n",
      "Epoch: 0033 loss_train: 1.7049 acc_train: 0.4714 acc_val: 0.5400 acc_test: 0.5560 time: 1.9527s\n",
      "Epoch: 0034 loss_train: 1.6631 acc_train: 0.5429 acc_val: 0.5480 acc_test: 0.5580 time: 2.0359s\n",
      "Epoch: 0035 loss_train: 1.6389 acc_train: 0.5857 acc_val: 0.6460 acc_test: 0.6550 time: 2.0052s\n",
      "Epoch: 0036 loss_train: 1.6004 acc_train: 0.6643 acc_val: 0.5140 acc_test: 0.5340 time: 2.0296s\n",
      "Epoch: 0037 loss_train: 1.6254 acc_train: 0.5429 acc_val: 0.5140 acc_test: 0.5380 time: 2.0624s\n",
      "Epoch: 0038 loss_train: 1.5859 acc_train: 0.6071 acc_val: 0.5640 acc_test: 0.5730 time: 2.0498s\n",
      "Epoch: 0039 loss_train: 1.5841 acc_train: 0.6214 acc_val: 0.5240 acc_test: 0.5330 time: 2.0546s\n",
      "Epoch: 0040 loss_train: 1.6077 acc_train: 0.5786 acc_val: 0.5240 acc_test: 0.5340 time: 2.0434s\n",
      "Epoch: 0041 loss_train: 1.6919 acc_train: 0.4429 acc_val: 0.5540 acc_test: 0.5870 time: 1.7161s\n",
      "Epoch: 0042 loss_train: 1.5970 acc_train: 0.5286 acc_val: 0.5340 acc_test: 0.5560 time: 1.5505s\n",
      "Epoch: 0043 loss_train: 1.5413 acc_train: 0.5500 acc_val: 0.5320 acc_test: 0.5590 time: 1.5331s\n",
      "Epoch: 0044 loss_train: 1.5799 acc_train: 0.5714 acc_val: 0.5380 acc_test: 0.5770 time: 1.5271s\n",
      "Epoch: 0045 loss_train: 1.5635 acc_train: 0.5357 acc_val: 0.5640 acc_test: 0.5800 time: 1.5302s\n",
      "Epoch: 0046 loss_train: 1.6977 acc_train: 0.4429 acc_val: 0.5680 acc_test: 0.5890 time: 1.5497s\n",
      "Epoch: 0047 loss_train: 1.6747 acc_train: 0.4500 acc_val: 0.7260 acc_test: 0.7450 time: 1.4705s\n",
      "Epoch: 0048 loss_train: 1.4735 acc_train: 0.6571 acc_val: 0.5560 acc_test: 0.5730 time: 1.4776s\n",
      "Epoch: 0049 loss_train: 1.5330 acc_train: 0.8286 acc_val: 0.6360 acc_test: 0.6570 time: 1.4768s\n",
      "Epoch: 0050 loss_train: 1.3831 acc_train: 0.7143 acc_val: 0.5680 acc_test: 0.5820 time: 1.5307s\n",
      "Epoch: 0051 loss_train: 1.4095 acc_train: 0.7071 acc_val: 0.5880 acc_test: 0.5880 time: 1.4788s\n",
      "Epoch: 0052 loss_train: 1.4222 acc_train: 0.6857 acc_val: 0.7460 acc_test: 0.7770 time: 1.5417s\n",
      "Epoch: 0053 loss_train: 1.4887 acc_train: 0.7714 acc_val: 0.5880 acc_test: 0.5890 time: 1.5410s\n",
      "Epoch: 0054 loss_train: 1.5035 acc_train: 0.5286 acc_val: 0.7000 acc_test: 0.7160 time: 1.4847s\n",
      "Epoch: 0055 loss_train: 1.3645 acc_train: 0.6500 acc_val: 0.5860 acc_test: 0.5890 time: 1.5697s\n",
      "Epoch: 0056 loss_train: 1.3802 acc_train: 0.5857 acc_val: 0.6380 acc_test: 0.6510 time: 1.5340s\n",
      "Epoch: 0057 loss_train: 1.3711 acc_train: 0.6929 acc_val: 0.5980 acc_test: 0.6190 time: 1.5343s\n",
      "Epoch: 0058 loss_train: 1.3549 acc_train: 0.5929 acc_val: 0.5940 acc_test: 0.6030 time: 1.5528s\n",
      "Epoch: 0059 loss_train: 1.3370 acc_train: 0.6500 acc_val: 0.7540 acc_test: 0.7520 time: 1.5375s\n",
      "Epoch: 0060 loss_train: 1.3880 acc_train: 0.6071 acc_val: 0.7560 acc_test: 0.7680 time: 1.5409s\n",
      "Epoch: 0061 loss_train: 1.2728 acc_train: 0.6357 acc_val: 0.6280 acc_test: 0.6290 time: 1.4849s\n",
      "Epoch: 0062 loss_train: 1.2178 acc_train: 0.7143 acc_val: 0.5960 acc_test: 0.6030 time: 1.5476s\n",
      "Epoch: 0063 loss_train: 1.1872 acc_train: 0.7286 acc_val: 0.7440 acc_test: 0.7670 time: 1.5378s\n",
      "Epoch: 0064 loss_train: 1.1869 acc_train: 0.6929 acc_val: 0.5940 acc_test: 0.6140 time: 1.5012s\n",
      "Epoch: 0065 loss_train: 1.2521 acc_train: 0.6214 acc_val: 0.6240 acc_test: 0.6320 time: 1.4752s\n",
      "Epoch: 0066 loss_train: 1.3171 acc_train: 0.6429 acc_val: 0.6760 acc_test: 0.6880 time: 1.4924s\n",
      "Epoch: 0067 loss_train: 1.2059 acc_train: 0.7000 acc_val: 0.6400 acc_test: 0.6540 time: 1.5673s\n",
      "Epoch: 0068 loss_train: 1.1972 acc_train: 0.8071 acc_val: 0.6000 acc_test: 0.6120 time: 1.5273s\n",
      "Epoch: 0069 loss_train: 1.1703 acc_train: 0.7429 acc_val: 0.6000 acc_test: 0.6050 time: 1.5448s\n",
      "Epoch: 0070 loss_train: 1.1607 acc_train: 0.7071 acc_val: 0.6060 acc_test: 0.6070 time: 1.5319s\n",
      "Epoch: 0071 loss_train: 1.3397 acc_train: 0.8071 acc_val: 0.7760 acc_test: 0.7930 time: 1.5549s\n",
      "Epoch: 0072 loss_train: 1.3540 acc_train: 0.6571 acc_val: 0.6060 acc_test: 0.6050 time: 1.5498s\n",
      "Epoch: 0073 loss_train: 1.3800 acc_train: 0.8500 acc_val: 0.6140 acc_test: 0.6260 time: 1.5409s\n",
      "Epoch: 0074 loss_train: 1.1658 acc_train: 0.6857 acc_val: 0.6580 acc_test: 0.6770 time: 1.5548s\n",
      "Epoch: 0075 loss_train: 1.0268 acc_train: 0.7571 acc_val: 0.6120 acc_test: 0.6110 time: 1.4883s\n",
      "Epoch: 0076 loss_train: 1.1141 acc_train: 0.7000 acc_val: 0.6520 acc_test: 0.6780 time: 1.5267s\n",
      "Epoch: 0077 loss_train: 1.3711 acc_train: 0.5786 acc_val: 0.6060 acc_test: 0.6180 time: 1.5463s\n",
      "Epoch: 0078 loss_train: 1.2922 acc_train: 0.8571 acc_val: 0.6060 acc_test: 0.6190 time: 1.5440s\n",
      "Epoch: 0079 loss_train: 1.1087 acc_train: 0.6643 acc_val: 0.7640 acc_test: 0.7830 time: 1.5372s\n",
      "Epoch: 0080 loss_train: 1.0770 acc_train: 0.7643 acc_val: 0.7860 acc_test: 0.8000 time: 1.5365s\n",
      "Epoch: 0081 loss_train: 1.0684 acc_train: 0.7286 acc_val: 0.6300 acc_test: 0.6350 time: 1.5468s\n",
      "Epoch: 0082 loss_train: 1.0114 acc_train: 0.7714 acc_val: 0.7040 acc_test: 0.7210 time: 1.5403s\n",
      "Epoch: 0083 loss_train: 0.9976 acc_train: 0.7500 acc_val: 0.6120 acc_test: 0.6190 time: 1.5419s\n",
      "Epoch: 0084 loss_train: 1.0566 acc_train: 0.7000 acc_val: 0.6180 acc_test: 0.6270 time: 1.5617s\n",
      "Epoch: 0085 loss_train: 1.0337 acc_train: 0.7214 acc_val: 0.6080 acc_test: 0.6250 time: 1.5563s\n",
      "Epoch: 0086 loss_train: 1.0025 acc_train: 0.7429 acc_val: 0.6140 acc_test: 0.6250 time: 1.5461s\n",
      "Epoch: 0087 loss_train: 0.9598 acc_train: 0.7143 acc_val: 0.6220 acc_test: 0.6490 time: 1.5476s\n",
      "Epoch: 0088 loss_train: 1.1863 acc_train: 0.9000 acc_val: 0.6740 acc_test: 0.6920 time: 1.5403s\n",
      "Epoch: 0089 loss_train: 1.1606 acc_train: 0.8857 acc_val: 0.6160 acc_test: 0.6350 time: 1.5234s\n",
      "Epoch: 0090 loss_train: 0.9757 acc_train: 0.7214 acc_val: 0.6100 acc_test: 0.6260 time: 1.5319s\n",
      "Epoch: 0091 loss_train: 0.9526 acc_train: 0.7643 acc_val: 0.6380 acc_test: 0.6410 time: 1.5368s\n",
      "Epoch: 0092 loss_train: 0.9806 acc_train: 0.7429 acc_val: 0.6780 acc_test: 0.6940 time: 1.5030s\n",
      "Epoch: 0093 loss_train: 1.0343 acc_train: 0.6786 acc_val: 0.7940 acc_test: 0.8070 time: 1.4880s\n",
      "Epoch: 0094 loss_train: 0.9930 acc_train: 0.6786 acc_val: 0.6420 acc_test: 0.6580 time: 1.5317s\n",
      "Epoch: 0095 loss_train: 0.9126 acc_train: 0.7714 acc_val: 0.6620 acc_test: 0.6760 time: 1.5456s\n",
      "Epoch: 0096 loss_train: 1.0020 acc_train: 0.7143 acc_val: 0.7060 acc_test: 0.7200 time: 1.5348s\n",
      "Epoch: 0097 loss_train: 0.8453 acc_train: 0.8214 acc_val: 0.6360 acc_test: 0.6500 time: 1.5565s\n",
      "Epoch: 0098 loss_train: 0.9276 acc_train: 0.8071 acc_val: 0.6200 acc_test: 0.6310 time: 1.5596s\n",
      "Epoch: 0099 loss_train: 1.1611 acc_train: 0.9071 acc_val: 0.6240 acc_test: 0.6340 time: 1.5324s\n",
      "Epoch: 0100 loss_train: 0.8833 acc_train: 0.7643 acc_val: 0.6200 acc_test: 0.6260 time: 1.5418s\n",
      "Epoch: 0101 loss_train: 0.9377 acc_train: 0.7357 acc_val: 0.6220 acc_test: 0.6280 time: 1.5658s\n",
      "Epoch: 0102 loss_train: 1.1474 acc_train: 0.8429 acc_val: 0.6780 acc_test: 0.7050 time: 1.5709s\n",
      "Epoch: 0103 loss_train: 0.9409 acc_train: 0.6929 acc_val: 0.6360 acc_test: 0.6330 time: 1.5006s\n",
      "Epoch: 0104 loss_train: 0.9495 acc_train: 0.7071 acc_val: 0.6380 acc_test: 0.6350 time: 1.5097s\n",
      "Epoch: 0105 loss_train: 0.9634 acc_train: 0.7071 acc_val: 0.6560 acc_test: 0.6730 time: 1.5274s\n",
      "Epoch: 0106 loss_train: 0.7552 acc_train: 0.8000 acc_val: 0.6380 acc_test: 0.6440 time: 1.5464s\n",
      "Epoch: 0107 loss_train: 1.1562 acc_train: 0.8786 acc_val: 0.6440 acc_test: 0.6500 time: 1.5328s\n",
      "Epoch: 0108 loss_train: 0.8501 acc_train: 0.7786 acc_val: 0.6300 acc_test: 0.6380 time: 1.5448s\n",
      "Epoch: 0109 loss_train: 0.8587 acc_train: 0.7714 acc_val: 0.6780 acc_test: 0.6910 time: 1.5500s\n",
      "Epoch: 0110 loss_train: 0.8878 acc_train: 0.7357 acc_val: 0.6360 acc_test: 0.6380 time: 1.5393s\n",
      "Epoch: 0111 loss_train: 0.8953 acc_train: 0.7500 acc_val: 0.6920 acc_test: 0.7070 time: 1.5558s\n",
      "Epoch: 0112 loss_train: 0.7654 acc_train: 0.8357 acc_val: 0.6420 acc_test: 0.6480 time: 1.5326s\n",
      "Epoch: 0113 loss_train: 0.9194 acc_train: 0.7357 acc_val: 0.6520 acc_test: 0.6620 time: 1.5363s\n",
      "Epoch: 0114 loss_train: 0.8437 acc_train: 0.7500 acc_val: 0.6320 acc_test: 0.6400 time: 1.5432s\n",
      "Epoch: 0115 loss_train: 0.8196 acc_train: 0.7929 acc_val: 0.6460 acc_test: 0.6610 time: 1.5533s\n",
      "Epoch: 0116 loss_train: 0.7559 acc_train: 0.8143 acc_val: 0.6480 acc_test: 0.6640 time: 1.5387s\n",
      "Epoch: 0117 loss_train: 0.7970 acc_train: 0.7643 acc_val: 0.6200 acc_test: 0.6320 time: 1.5507s\n",
      "Epoch: 0118 loss_train: 0.7433 acc_train: 0.8500 acc_val: 0.6320 acc_test: 0.6580 time: 1.5051s\n",
      "Epoch: 0119 loss_train: 0.8234 acc_train: 0.7929 acc_val: 0.6700 acc_test: 0.7070 time: 1.4926s\n",
      "Epoch: 0120 loss_train: 0.8039 acc_train: 0.8071 acc_val: 0.6300 acc_test: 0.6400 time: 1.5030s\n",
      "Epoch: 0121 loss_train: 0.9344 acc_train: 0.7071 acc_val: 0.6320 acc_test: 0.6400 time: 1.5545s\n",
      "Epoch: 0122 loss_train: 0.7934 acc_train: 0.7643 acc_val: 0.7100 acc_test: 0.7460 time: 1.5417s\n",
      "Epoch: 0123 loss_train: 0.8186 acc_train: 0.7714 acc_val: 0.8020 acc_test: 0.8080 time: 1.5434s\n",
      "Epoch: 0124 loss_train: 0.7178 acc_train: 0.7929 acc_val: 0.6480 acc_test: 0.6580 time: 1.5330s\n",
      "Epoch: 0125 loss_train: 0.8215 acc_train: 0.7429 acc_val: 0.6440 acc_test: 0.6450 time: 1.5519s\n",
      "Epoch: 0126 loss_train: 0.8732 acc_train: 0.7429 acc_val: 0.6740 acc_test: 0.6760 time: 1.5441s\n",
      "Epoch: 0127 loss_train: 0.8229 acc_train: 0.7500 acc_val: 0.6360 acc_test: 0.6440 time: 1.5440s\n",
      "Epoch: 0128 loss_train: 1.1226 acc_train: 0.6643 acc_val: 0.6680 acc_test: 0.6790 time: 1.5512s\n",
      "Epoch: 0129 loss_train: 0.8677 acc_train: 0.9143 acc_val: 0.7700 acc_test: 0.7910 time: 1.5453s\n",
      "Epoch: 0130 loss_train: 0.7414 acc_train: 0.7857 acc_val: 0.7180 acc_test: 0.7580 time: 1.5460s\n",
      "Epoch: 0131 loss_train: 0.7582 acc_train: 0.8000 acc_val: 0.6140 acc_test: 0.6350 time: 1.5384s\n",
      "Epoch: 0132 loss_train: 0.9111 acc_train: 0.7357 acc_val: 0.5960 acc_test: 0.6170 time: 1.5455s\n",
      "Epoch: 0133 loss_train: 0.7122 acc_train: 0.8429 acc_val: 0.5980 acc_test: 0.6150 time: 1.5554s\n",
      "Epoch: 0134 loss_train: 0.7971 acc_train: 0.9214 acc_val: 0.5980 acc_test: 0.6060 time: 1.5474s\n",
      "Epoch: 0135 loss_train: 0.7195 acc_train: 0.8357 acc_val: 0.6040 acc_test: 0.6130 time: 1.5517s\n",
      "Epoch: 0136 loss_train: 0.7490 acc_train: 0.7571 acc_val: 0.6160 acc_test: 0.6280 time: 1.5557s\n",
      "Epoch: 0137 loss_train: 0.7699 acc_train: 0.7714 acc_val: 0.6840 acc_test: 0.7170 time: 1.5515s\n",
      "Epoch: 0138 loss_train: 0.6825 acc_train: 0.8714 acc_val: 0.7000 acc_test: 0.7370 time: 1.5432s\n",
      "Epoch: 0139 loss_train: 0.7421 acc_train: 0.8214 acc_val: 0.7960 acc_test: 0.8050 time: 1.5543s\n",
      "Epoch: 0140 loss_train: 0.6712 acc_train: 0.8643 acc_val: 0.6420 acc_test: 0.6460 time: 1.5066s\n",
      "Epoch: 0141 loss_train: 0.7758 acc_train: 0.7857 acc_val: 0.6500 acc_test: 0.6640 time: 1.5305s\n",
      "Epoch: 0142 loss_train: 0.8745 acc_train: 0.7500 acc_val: 0.6840 acc_test: 0.6970 time: 1.5415s\n",
      "Epoch: 0143 loss_train: 0.7016 acc_train: 0.8214 acc_val: 0.6460 acc_test: 0.6560 time: 1.5416s\n",
      "Epoch: 0144 loss_train: 0.8994 acc_train: 0.7571 acc_val: 0.6440 acc_test: 0.6500 time: 1.5371s\n",
      "Epoch: 0145 loss_train: 0.7110 acc_train: 0.8143 acc_val: 0.6440 acc_test: 0.6450 time: 1.5376s\n",
      "Epoch: 0146 loss_train: 0.7294 acc_train: 0.7786 acc_val: 0.6420 acc_test: 0.6440 time: 1.5469s\n",
      "Epoch: 0147 loss_train: 0.7551 acc_train: 0.7786 acc_val: 0.6800 acc_test: 0.6890 time: 1.5362s\n",
      "Epoch: 0148 loss_train: 0.6299 acc_train: 0.8429 acc_val: 0.6500 acc_test: 0.6510 time: 1.5363s\n",
      "Epoch: 0149 loss_train: 0.6263 acc_train: 0.8429 acc_val: 0.6660 acc_test: 0.7060 time: 1.5637s\n",
      "Epoch: 0150 loss_train: 0.7487 acc_train: 0.7857 acc_val: 0.6380 acc_test: 0.6450 time: 1.5326s\n",
      "Epoch: 0151 loss_train: 0.7208 acc_train: 0.8357 acc_val: 0.7780 acc_test: 0.7900 time: 1.5596s\n",
      "Epoch: 0152 loss_train: 0.6054 acc_train: 0.9000 acc_val: 0.6300 acc_test: 0.6480 time: 1.5487s\n",
      "Epoch: 0153 loss_train: 0.6821 acc_train: 0.8071 acc_val: 0.6480 acc_test: 0.6720 time: 1.5435s\n",
      "Epoch: 0154 loss_train: 0.6299 acc_train: 0.8214 acc_val: 0.7800 acc_test: 0.8050 time: 1.5632s\n",
      "Epoch: 0155 loss_train: 0.7304 acc_train: 0.9000 acc_val: 0.7920 acc_test: 0.7990 time: 1.5103s\n",
      "Epoch: 0156 loss_train: 0.6758 acc_train: 0.8357 acc_val: 0.6320 acc_test: 0.6420 time: 1.4919s\n",
      "Epoch: 0157 loss_train: 0.6473 acc_train: 0.8071 acc_val: 0.6380 acc_test: 0.6450 time: 1.4884s\n",
      "Epoch: 0158 loss_train: 0.6353 acc_train: 0.8643 acc_val: 0.6380 acc_test: 0.6340 time: 1.5640s\n",
      "Epoch: 0159 loss_train: 0.6644 acc_train: 0.8000 acc_val: 0.6440 acc_test: 0.6470 time: 1.5421s\n",
      "Epoch: 0160 loss_train: 0.7312 acc_train: 0.7643 acc_val: 0.6380 acc_test: 0.6360 time: 1.5497s\n",
      "Epoch: 0161 loss_train: 0.6935 acc_train: 0.7857 acc_val: 0.7920 acc_test: 0.8010 time: 1.5624s\n",
      "Epoch: 0162 loss_train: 0.6995 acc_train: 0.8071 acc_val: 0.6760 acc_test: 0.6850 time: 1.5628s\n",
      "Epoch: 0163 loss_train: 0.6399 acc_train: 0.8571 acc_val: 0.6460 acc_test: 0.6420 time: 1.5518s\n",
      "Epoch: 0164 loss_train: 0.6772 acc_train: 0.8643 acc_val: 0.6500 acc_test: 0.6460 time: 1.5584s\n",
      "Epoch: 0165 loss_train: 0.8875 acc_train: 0.9000 acc_val: 0.6820 acc_test: 0.6990 time: 1.5402s\n",
      "Epoch: 0166 loss_train: 0.6507 acc_train: 0.9357 acc_val: 0.6420 acc_test: 0.6320 time: 1.4834s\n",
      "Epoch: 0167 loss_train: 0.6798 acc_train: 0.8643 acc_val: 0.7700 acc_test: 0.7790 time: 1.4792s\n",
      "Epoch: 0168 loss_train: 0.6930 acc_train: 0.8214 acc_val: 0.6300 acc_test: 0.6260 time: 1.4773s\n",
      "Epoch: 0169 loss_train: 0.6421 acc_train: 0.8857 acc_val: 0.7640 acc_test: 0.7780 time: 1.4975s\n",
      "Epoch: 0170 loss_train: 0.6457 acc_train: 0.9214 acc_val: 0.6620 acc_test: 0.6800 time: 1.5448s\n",
      "Epoch: 0171 loss_train: 0.5707 acc_train: 0.8643 acc_val: 0.6300 acc_test: 0.6430 time: 1.5494s\n",
      "Epoch: 0172 loss_train: 0.6830 acc_train: 0.8286 acc_val: 0.6100 acc_test: 0.6200 time: 1.5403s\n",
      "Epoch: 0173 loss_train: 0.6333 acc_train: 0.8357 acc_val: 0.6100 acc_test: 0.6160 time: 1.5492s\n",
      "Epoch: 0174 loss_train: 0.6576 acc_train: 0.7929 acc_val: 0.6160 acc_test: 0.6200 time: 1.5284s\n",
      "Epoch: 0175 loss_train: 0.6559 acc_train: 0.8286 acc_val: 0.6100 acc_test: 0.6160 time: 1.5615s\n",
      "Epoch: 0176 loss_train: 0.9647 acc_train: 0.7214 acc_val: 0.6160 acc_test: 0.6130 time: 1.5450s\n",
      "Epoch: 0177 loss_train: 0.6510 acc_train: 0.8286 acc_val: 0.6180 acc_test: 0.6150 time: 1.5496s\n",
      "Epoch: 0178 loss_train: 0.6730 acc_train: 0.8143 acc_val: 0.6240 acc_test: 0.6270 time: 1.5364s\n",
      "Epoch: 0179 loss_train: 0.6462 acc_train: 0.8214 acc_val: 0.6320 acc_test: 0.6290 time: 1.5398s\n",
      "Epoch: 0180 loss_train: 0.6490 acc_train: 0.8286 acc_val: 0.6720 acc_test: 0.6680 time: 1.5468s\n",
      "Epoch: 0181 loss_train: 0.9114 acc_train: 0.7929 acc_val: 0.6400 acc_test: 0.6260 time: 1.5367s\n",
      "Epoch: 0182 loss_train: 0.6716 acc_train: 0.8143 acc_val: 0.6400 acc_test: 0.6260 time: 1.5178s\n",
      "Epoch: 0183 loss_train: 0.6104 acc_train: 0.8571 acc_val: 0.7200 acc_test: 0.7440 time: 1.4822s\n",
      "Epoch: 0184 loss_train: 0.9173 acc_train: 0.7571 acc_val: 0.6460 acc_test: 0.6380 time: 1.5448s\n",
      "Epoch: 0185 loss_train: 0.5861 acc_train: 0.8357 acc_val: 0.7900 acc_test: 0.7980 time: 1.5524s\n",
      "Epoch: 0186 loss_train: 0.6692 acc_train: 0.7786 acc_val: 0.6420 acc_test: 0.6340 time: 1.5544s\n",
      "Epoch: 0187 loss_train: 0.6584 acc_train: 0.7857 acc_val: 0.7840 acc_test: 0.8030 time: 1.5614s\n",
      "Epoch: 0188 loss_train: 0.8059 acc_train: 0.9500 acc_val: 0.6420 acc_test: 0.6410 time: 1.5489s\n",
      "Epoch: 0189 loss_train: 0.5727 acc_train: 0.8929 acc_val: 0.7940 acc_test: 0.8020 time: 1.5536s\n",
      "Epoch: 0190 loss_train: 0.6425 acc_train: 0.8214 acc_val: 0.6360 acc_test: 0.6400 time: 1.5595s\n",
      "Epoch: 0191 loss_train: 0.6466 acc_train: 0.8357 acc_val: 0.6340 acc_test: 0.6410 time: 1.5598s\n",
      "Epoch: 0192 loss_train: 0.5351 acc_train: 0.8429 acc_val: 0.6320 acc_test: 0.6430 time: 1.5435s\n",
      "Epoch: 0193 loss_train: 0.6084 acc_train: 0.8571 acc_val: 0.6340 acc_test: 0.6420 time: 1.5430s\n",
      "Epoch: 0194 loss_train: 0.6695 acc_train: 0.8500 acc_val: 0.6420 acc_test: 0.6580 time: 1.5724s\n",
      "Epoch: 0195 loss_train: 0.7197 acc_train: 0.8786 acc_val: 0.6400 acc_test: 0.6460 time: 1.5370s\n",
      "Epoch: 0196 loss_train: 0.6116 acc_train: 0.8714 acc_val: 0.6520 acc_test: 0.6550 time: 1.5389s\n",
      "Epoch: 0197 loss_train: 0.6905 acc_train: 0.7929 acc_val: 0.8020 acc_test: 0.8010 time: 1.5428s\n",
      "Epoch: 0198 loss_train: 0.6107 acc_train: 0.8357 acc_val: 0.6420 acc_test: 0.6470 time: 1.5503s\n",
      "Epoch: 0199 loss_train: 0.6182 acc_train: 0.8000 acc_val: 0.6560 acc_test: 0.6550 time: 1.5406s\n",
      "Epoch: 0200 loss_train: 0.5577 acc_train: 0.8214 acc_val: 0.6480 acc_test: 0.6450 time: 1.5081s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 335.8140s\n",
      "0.802 0.801\n",
      "0.796 0.805\n",
      "0.794 0.802\n",
      "0.792 0.801\n",
      "0.79 0.798\n",
      "0.786 0.8\n",
      "0.784 0.803\n",
      "0.78 0.805\n",
      "0.778 0.79\n",
      "0.776 0.793\n",
      "parameters setting here\n",
      "features.shape torch.Size([2708, 1433])\n",
      "labels.max().item() 6\n",
      "Epoch: 0001 loss_train: 1.9795 acc_train: 0.1429 acc_val: 0.0720 acc_test: 0.0910 time: 1.5424s\n",
      "Epoch: 0002 loss_train: 1.9797 acc_train: 0.1500 acc_val: 0.0720 acc_test: 0.0910 time: 1.4891s\n",
      "Epoch: 0003 loss_train: 1.9692 acc_train: 0.1357 acc_val: 0.0720 acc_test: 0.0910 time: 1.6279s\n",
      "Epoch: 0004 loss_train: 1.9831 acc_train: 0.1071 acc_val: 0.0720 acc_test: 0.0910 time: 1.5112s\n",
      "Epoch: 0005 loss_train: 1.9398 acc_train: 0.1500 acc_val: 0.0720 acc_test: 0.0910 time: 1.5018s\n",
      "Epoch: 0006 loss_train: 1.9486 acc_train: 0.1786 acc_val: 0.0740 acc_test: 0.0910 time: 1.5020s\n",
      "Epoch: 0007 loss_train: 1.9386 acc_train: 0.1857 acc_val: 0.0740 acc_test: 0.0910 time: 1.5104s\n",
      "Epoch: 0008 loss_train: 1.9357 acc_train: 0.1357 acc_val: 0.0740 acc_test: 0.0920 time: 1.5076s\n",
      "Epoch: 0009 loss_train: 1.9153 acc_train: 0.2143 acc_val: 0.0740 acc_test: 0.0920 time: 1.5103s\n",
      "Epoch: 0010 loss_train: 1.9334 acc_train: 0.2000 acc_val: 0.0920 acc_test: 0.1080 time: 1.5384s\n",
      "Epoch: 0011 loss_train: 1.9234 acc_train: 0.2286 acc_val: 0.1260 acc_test: 0.1330 time: 1.4955s\n",
      "Epoch: 0012 loss_train: 1.9039 acc_train: 0.2357 acc_val: 0.1660 acc_test: 0.1740 time: 1.5118s\n",
      "Epoch: 0013 loss_train: 1.9267 acc_train: 0.2143 acc_val: 0.1820 acc_test: 0.1930 time: 1.5142s\n",
      "Epoch: 0014 loss_train: 1.9094 acc_train: 0.2214 acc_val: 0.1940 acc_test: 0.2100 time: 1.5136s\n",
      "Epoch: 0015 loss_train: 1.9147 acc_train: 0.2429 acc_val: 0.1920 acc_test: 0.2070 time: 1.5013s\n",
      "Epoch: 0016 loss_train: 1.8800 acc_train: 0.2643 acc_val: 0.2800 acc_test: 0.2640 time: 1.5108s\n",
      "Epoch: 0017 loss_train: 1.8721 acc_train: 0.2929 acc_val: 0.3180 acc_test: 0.3050 time: 1.5381s\n",
      "Epoch: 0018 loss_train: 1.8638 acc_train: 0.3429 acc_val: 0.3540 acc_test: 0.3330 time: 1.5199s\n",
      "Epoch: 0019 loss_train: 1.8779 acc_train: 0.3143 acc_val: 0.3820 acc_test: 0.3650 time: 1.5361s\n",
      "Epoch: 0020 loss_train: 1.8736 acc_train: 0.2500 acc_val: 0.4840 acc_test: 0.4860 time: 1.5127s\n",
      "Epoch: 0021 loss_train: 1.8773 acc_train: 0.2857 acc_val: 0.4340 acc_test: 0.4090 time: 1.5009s\n",
      "Epoch: 0022 loss_train: 1.8524 acc_train: 0.2857 acc_val: 0.4440 acc_test: 0.4300 time: 1.5491s\n",
      "Epoch: 0023 loss_train: 1.8376 acc_train: 0.3857 acc_val: 0.4380 acc_test: 0.4390 time: 1.4955s\n",
      "Epoch: 0024 loss_train: 1.8345 acc_train: 0.3929 acc_val: 0.5160 acc_test: 0.5330 time: 1.5168s\n",
      "Epoch: 0025 loss_train: 1.8594 acc_train: 0.2714 acc_val: 0.5120 acc_test: 0.5330 time: 1.5441s\n",
      "Epoch: 0026 loss_train: 1.8585 acc_train: 0.3000 acc_val: 0.4500 acc_test: 0.4500 time: 1.4855s\n",
      "Epoch: 0027 loss_train: 1.8397 acc_train: 0.3571 acc_val: 0.4780 acc_test: 0.4730 time: 1.5413s\n",
      "Epoch: 0028 loss_train: 1.7955 acc_train: 0.4571 acc_val: 0.4840 acc_test: 0.4770 time: 1.5449s\n",
      "Epoch: 0029 loss_train: 1.8638 acc_train: 0.5571 acc_val: 0.5000 acc_test: 0.4790 time: 1.6360s\n",
      "Epoch: 0030 loss_train: 1.7663 acc_train: 0.4714 acc_val: 0.4940 acc_test: 0.4730 time: 1.5139s\n",
      "Epoch: 0031 loss_train: 1.7550 acc_train: 0.4929 acc_val: 0.5020 acc_test: 0.4780 time: 1.5569s\n",
      "Epoch: 0032 loss_train: 1.7440 acc_train: 0.5714 acc_val: 0.5080 acc_test: 0.4930 time: 1.5297s\n",
      "Epoch: 0033 loss_train: 1.7220 acc_train: 0.5714 acc_val: 0.4840 acc_test: 0.4640 time: 1.5093s\n",
      "Epoch: 0034 loss_train: 1.7099 acc_train: 0.5643 acc_val: 0.4760 acc_test: 0.4520 time: 1.5170s\n",
      "Epoch: 0035 loss_train: 1.6948 acc_train: 0.5500 acc_val: 0.5020 acc_test: 0.4600 time: 1.5490s\n",
      "Epoch: 0036 loss_train: 1.6975 acc_train: 0.5643 acc_val: 0.4760 acc_test: 0.4340 time: 1.4838s\n",
      "Epoch: 0037 loss_train: 1.6939 acc_train: 0.5071 acc_val: 0.4740 acc_test: 0.4300 time: 1.5278s\n",
      "Epoch: 0038 loss_train: 1.7933 acc_train: 0.6214 acc_val: 0.4680 acc_test: 0.4210 time: 1.5499s\n",
      "Epoch: 0039 loss_train: 1.7301 acc_train: 0.4071 acc_val: 0.4680 acc_test: 0.4230 time: 1.5409s\n",
      "Epoch: 0040 loss_train: 1.6075 acc_train: 0.5429 acc_val: 0.4780 acc_test: 0.4260 time: 1.5164s\n",
      "Epoch: 0041 loss_train: 1.6297 acc_train: 0.5071 acc_val: 0.4800 acc_test: 0.4280 time: 1.5213s\n",
      "Epoch: 0042 loss_train: 1.6263 acc_train: 0.5571 acc_val: 0.4860 acc_test: 0.4440 time: 1.5097s\n",
      "Epoch: 0043 loss_train: 1.6178 acc_train: 0.5357 acc_val: 0.5060 acc_test: 0.4620 time: 1.5099s\n",
      "Epoch: 0044 loss_train: 1.7053 acc_train: 0.7357 acc_val: 0.5100 acc_test: 0.4840 time: 1.5521s\n",
      "Epoch: 0045 loss_train: 1.6088 acc_train: 0.5214 acc_val: 0.5460 acc_test: 0.5210 time: 1.5458s\n",
      "Epoch: 0046 loss_train: 1.5301 acc_train: 0.5643 acc_val: 0.5440 acc_test: 0.5330 time: 1.5018s\n",
      "Epoch: 0047 loss_train: 1.5332 acc_train: 0.6071 acc_val: 0.5700 acc_test: 0.5620 time: 1.5241s\n",
      "Epoch: 0048 loss_train: 1.5132 acc_train: 0.5929 acc_val: 0.6000 acc_test: 0.5890 time: 1.5364s\n",
      "Epoch: 0049 loss_train: 1.5110 acc_train: 0.6357 acc_val: 0.6120 acc_test: 0.6060 time: 1.4253s\n",
      "Epoch: 0050 loss_train: 1.5786 acc_train: 0.5714 acc_val: 0.6540 acc_test: 0.6520 time: 1.5828s\n",
      "Epoch: 0051 loss_train: 1.5296 acc_train: 0.5500 acc_val: 0.6540 acc_test: 0.6550 time: 1.4933s\n",
      "Epoch: 0052 loss_train: 1.4546 acc_train: 0.6857 acc_val: 0.7420 acc_test: 0.7610 time: 1.5800s\n",
      "Epoch: 0053 loss_train: 1.4287 acc_train: 0.6357 acc_val: 0.6780 acc_test: 0.6780 time: 1.5751s\n",
      "Epoch: 0054 loss_train: 1.4571 acc_train: 0.6714 acc_val: 0.6200 acc_test: 0.6220 time: 1.4658s\n",
      "Epoch: 0055 loss_train: 1.3778 acc_train: 0.7000 acc_val: 0.7020 acc_test: 0.7140 time: 1.5410s\n",
      "Epoch: 0056 loss_train: 1.4532 acc_train: 0.7786 acc_val: 0.6320 acc_test: 0.6300 time: 1.5442s\n",
      "Epoch: 0057 loss_train: 1.4297 acc_train: 0.6429 acc_val: 0.6140 acc_test: 0.6260 time: 1.4401s\n",
      "Epoch: 0058 loss_train: 1.3340 acc_train: 0.6643 acc_val: 0.6180 acc_test: 0.6280 time: 1.5189s\n",
      "Epoch: 0059 loss_train: 1.3690 acc_train: 0.6500 acc_val: 0.6240 acc_test: 0.6350 time: 1.4992s\n",
      "Epoch: 0060 loss_train: 1.3096 acc_train: 0.6571 acc_val: 0.7060 acc_test: 0.7220 time: 1.5124s\n",
      "Epoch: 0061 loss_train: 1.2826 acc_train: 0.7214 acc_val: 0.6200 acc_test: 0.6180 time: 1.5586s\n",
      "Epoch: 0062 loss_train: 1.3105 acc_train: 0.6429 acc_val: 0.6180 acc_test: 0.6080 time: 1.5143s\n",
      "Epoch: 0063 loss_train: 1.2351 acc_train: 0.7357 acc_val: 0.7560 acc_test: 0.7640 time: 1.5260s\n",
      "Epoch: 0064 loss_train: 1.4703 acc_train: 0.5429 acc_val: 0.7740 acc_test: 0.7710 time: 1.5003s\n",
      "Epoch: 0065 loss_train: 1.2533 acc_train: 0.7500 acc_val: 0.6100 acc_test: 0.6160 time: 1.5022s\n",
      "Epoch: 0066 loss_train: 1.3182 acc_train: 0.6571 acc_val: 0.7740 acc_test: 0.7770 time: 1.5094s\n",
      "Epoch: 0067 loss_train: 1.2535 acc_train: 0.6786 acc_val: 0.6140 acc_test: 0.6310 time: 1.5404s\n",
      "Epoch: 0068 loss_train: 1.2876 acc_train: 0.8286 acc_val: 0.6160 acc_test: 0.6390 time: 1.5729s\n",
      "Epoch: 0069 loss_train: 1.1632 acc_train: 0.7929 acc_val: 0.6600 acc_test: 0.6830 time: 1.4987s\n",
      "Epoch: 0070 loss_train: 1.1994 acc_train: 0.7286 acc_val: 0.6220 acc_test: 0.6330 time: 1.4884s\n",
      "Epoch: 0071 loss_train: 1.1017 acc_train: 0.7929 acc_val: 0.6200 acc_test: 0.6320 time: 1.4885s\n",
      "Epoch: 0072 loss_train: 1.2509 acc_train: 0.6929 acc_val: 0.6280 acc_test: 0.6370 time: 1.5101s\n",
      "Epoch: 0073 loss_train: 1.1811 acc_train: 0.7071 acc_val: 0.6340 acc_test: 0.6470 time: 1.4996s\n",
      "Epoch: 0074 loss_train: 1.1078 acc_train: 0.7143 acc_val: 0.6380 acc_test: 0.6490 time: 1.5490s\n",
      "Epoch: 0075 loss_train: 1.1074 acc_train: 0.7643 acc_val: 0.6600 acc_test: 0.6890 time: 1.5222s\n",
      "Epoch: 0076 loss_train: 1.0811 acc_train: 0.7429 acc_val: 0.6340 acc_test: 0.6370 time: 1.4876s\n",
      "Epoch: 0077 loss_train: 1.3310 acc_train: 0.6357 acc_val: 0.6560 acc_test: 0.6840 time: 1.4293s\n",
      "Epoch: 0078 loss_train: 1.1004 acc_train: 0.6857 acc_val: 0.7760 acc_test: 0.7900 time: 1.4968s\n",
      "Epoch: 0079 loss_train: 1.0809 acc_train: 0.6857 acc_val: 0.6340 acc_test: 0.6270 time: 1.5572s\n",
      "Epoch: 0080 loss_train: 0.9770 acc_train: 0.8000 acc_val: 0.7900 acc_test: 0.7970 time: 1.5764s\n",
      "Epoch: 0081 loss_train: 1.0937 acc_train: 0.6857 acc_val: 0.7800 acc_test: 0.7910 time: 1.5523s\n",
      "Epoch: 0082 loss_train: 1.0615 acc_train: 0.7143 acc_val: 0.6400 acc_test: 0.6270 time: 1.5366s\n",
      "Epoch: 0083 loss_train: 0.9793 acc_train: 0.7500 acc_val: 0.6380 acc_test: 0.6200 time: 1.5021s\n",
      "Epoch: 0084 loss_train: 1.1851 acc_train: 0.8429 acc_val: 0.6320 acc_test: 0.6180 time: 1.5078s\n",
      "Epoch: 0085 loss_train: 1.0613 acc_train: 0.7929 acc_val: 0.6340 acc_test: 0.6230 time: 1.5320s\n",
      "Epoch: 0086 loss_train: 1.0895 acc_train: 0.7500 acc_val: 0.6280 acc_test: 0.6240 time: 1.5146s\n",
      "Epoch: 0087 loss_train: 1.0530 acc_train: 0.7929 acc_val: 0.6300 acc_test: 0.6330 time: 1.5010s\n",
      "Epoch: 0088 loss_train: 1.0860 acc_train: 0.7000 acc_val: 0.6420 acc_test: 0.6400 time: 1.4997s\n",
      "Epoch: 0089 loss_train: 0.9999 acc_train: 0.7000 acc_val: 0.6380 acc_test: 0.6340 time: 1.5065s\n",
      "Epoch: 0090 loss_train: 0.9501 acc_train: 0.7571 acc_val: 0.6340 acc_test: 0.6370 time: 1.5069s\n",
      "Epoch: 0091 loss_train: 0.9995 acc_train: 0.7071 acc_val: 0.7540 acc_test: 0.7560 time: 1.5051s\n",
      "Epoch: 0092 loss_train: 0.9570 acc_train: 0.7143 acc_val: 0.6380 acc_test: 0.6340 time: 1.5102s\n",
      "Epoch: 0093 loss_train: 0.9687 acc_train: 0.7357 acc_val: 0.7800 acc_test: 0.7800 time: 1.5579s\n",
      "Epoch: 0094 loss_train: 0.9415 acc_train: 0.7571 acc_val: 0.6340 acc_test: 0.6350 time: 1.5254s\n",
      "Epoch: 0095 loss_train: 0.9760 acc_train: 0.7714 acc_val: 0.6620 acc_test: 0.6690 time: 1.5046s\n",
      "Epoch: 0096 loss_train: 0.8795 acc_train: 0.7929 acc_val: 0.6580 acc_test: 0.6510 time: 1.5284s\n",
      "Epoch: 0097 loss_train: 0.9180 acc_train: 0.7357 acc_val: 0.7360 acc_test: 0.7660 time: 1.5368s\n",
      "Epoch: 0098 loss_train: 0.9521 acc_train: 0.7643 acc_val: 0.6500 acc_test: 0.6370 time: 1.5614s\n",
      "Epoch: 0099 loss_train: 1.0844 acc_train: 0.8857 acc_val: 0.6460 acc_test: 0.6280 time: 1.5161s\n",
      "Epoch: 0100 loss_train: 0.9739 acc_train: 0.7929 acc_val: 0.6440 acc_test: 0.6240 time: 1.5033s\n",
      "Epoch: 0101 loss_train: 0.9484 acc_train: 0.8429 acc_val: 0.6500 acc_test: 0.6380 time: 1.5430s\n",
      "Epoch: 0102 loss_train: 0.9609 acc_train: 0.7214 acc_val: 0.7080 acc_test: 0.7100 time: 1.5005s\n",
      "Epoch: 0103 loss_train: 0.9045 acc_train: 0.7500 acc_val: 0.6540 acc_test: 0.6410 time: 1.5046s\n",
      "Epoch: 0104 loss_train: 0.9043 acc_train: 0.7929 acc_val: 0.7260 acc_test: 0.7370 time: 1.5610s\n",
      "Epoch: 0105 loss_train: 0.8528 acc_train: 0.7786 acc_val: 0.7920 acc_test: 0.8000 time: 1.4674s\n",
      "Epoch: 0106 loss_train: 0.9215 acc_train: 0.7286 acc_val: 0.6420 acc_test: 0.6320 time: 1.4930s\n",
      "Epoch: 0107 loss_train: 0.9008 acc_train: 0.7429 acc_val: 0.6360 acc_test: 0.6280 time: 1.5130s\n",
      "Epoch: 0108 loss_train: 0.8894 acc_train: 0.7286 acc_val: 0.6620 acc_test: 0.6660 time: 1.5267s\n",
      "Epoch: 0109 loss_train: 0.8566 acc_train: 0.7571 acc_val: 0.6460 acc_test: 0.6310 time: 1.5155s\n",
      "Epoch: 0110 loss_train: 0.8079 acc_train: 0.8357 acc_val: 0.6380 acc_test: 0.6290 time: 1.5012s\n",
      "Epoch: 0111 loss_train: 0.9941 acc_train: 0.8643 acc_val: 0.6720 acc_test: 0.6850 time: 1.5132s\n",
      "Epoch: 0112 loss_train: 0.8551 acc_train: 0.8286 acc_val: 0.6460 acc_test: 0.6400 time: 1.5075s\n",
      "Epoch: 0113 loss_train: 0.8539 acc_train: 0.7357 acc_val: 0.6280 acc_test: 0.6260 time: 1.5530s\n",
      "Epoch: 0114 loss_train: 0.8404 acc_train: 0.8143 acc_val: 0.7860 acc_test: 0.7880 time: 1.4571s\n",
      "Epoch: 0115 loss_train: 0.8325 acc_train: 0.7571 acc_val: 0.6400 acc_test: 0.6490 time: 1.5264s\n",
      "Epoch: 0116 loss_train: 0.8773 acc_train: 0.7500 acc_val: 0.6120 acc_test: 0.6180 time: 1.5080s\n",
      "Epoch: 0117 loss_train: 0.8379 acc_train: 0.7643 acc_val: 0.6120 acc_test: 0.6180 time: 1.4957s\n",
      "Epoch: 0118 loss_train: 0.7442 acc_train: 0.8071 acc_val: 0.6560 acc_test: 0.6730 time: 1.5069s\n",
      "Epoch: 0119 loss_train: 0.7655 acc_train: 0.8214 acc_val: 0.6560 acc_test: 0.6560 time: 1.5169s\n",
      "Epoch: 0120 loss_train: 0.8239 acc_train: 0.8071 acc_val: 0.6140 acc_test: 0.6290 time: 1.5564s\n",
      "Epoch: 0121 loss_train: 0.8163 acc_train: 0.7643 acc_val: 0.6200 acc_test: 0.6360 time: 1.4963s\n",
      "Epoch: 0122 loss_train: 1.0323 acc_train: 0.6857 acc_val: 0.6560 acc_test: 0.6710 time: 1.5070s\n",
      "Epoch: 0123 loss_train: 0.8896 acc_train: 0.7786 acc_val: 0.7900 acc_test: 0.7940 time: 1.5497s\n",
      "Epoch: 0124 loss_train: 0.9795 acc_train: 0.9000 acc_val: 0.6900 acc_test: 0.7100 time: 1.4708s\n",
      "Epoch: 0125 loss_train: 0.7014 acc_train: 0.8357 acc_val: 0.7140 acc_test: 0.7310 time: 1.5417s\n",
      "Epoch: 0126 loss_train: 0.7360 acc_train: 0.8286 acc_val: 0.8060 acc_test: 0.8050 time: 1.5517s\n",
      "Epoch: 0127 loss_train: 0.7472 acc_train: 0.8357 acc_val: 0.6500 acc_test: 0.6480 time: 1.4979s\n",
      "Epoch: 0128 loss_train: 0.7177 acc_train: 0.8357 acc_val: 0.6360 acc_test: 0.6380 time: 1.4660s\n",
      "Epoch: 0129 loss_train: 0.6506 acc_train: 0.8786 acc_val: 0.8000 acc_test: 0.8080 time: 1.4749s\n",
      "Epoch: 0130 loss_train: 0.7734 acc_train: 0.8071 acc_val: 0.8080 acc_test: 0.8150 time: 1.5582s\n",
      "Epoch: 0131 loss_train: 0.7959 acc_train: 0.7857 acc_val: 0.6500 acc_test: 0.6480 time: 1.5340s\n",
      "Epoch: 0132 loss_train: 0.7019 acc_train: 0.8429 acc_val: 0.6980 acc_test: 0.7130 time: 1.5070s\n",
      "Epoch: 0133 loss_train: 0.7865 acc_train: 0.8357 acc_val: 0.6660 acc_test: 0.6720 time: 1.5704s\n",
      "Epoch: 0134 loss_train: 0.8136 acc_train: 0.7929 acc_val: 0.6920 acc_test: 0.6980 time: 1.6716s\n",
      "Epoch: 0135 loss_train: 0.7938 acc_train: 0.7500 acc_val: 0.6220 acc_test: 0.6320 time: 1.5029s\n",
      "Epoch: 0136 loss_train: 0.7327 acc_train: 0.9286 acc_val: 0.6240 acc_test: 0.6370 time: 1.4889s\n",
      "Epoch: 0137 loss_train: 0.6754 acc_train: 0.8143 acc_val: 0.7880 acc_test: 0.7880 time: 1.4904s\n",
      "Epoch: 0138 loss_train: 0.7356 acc_train: 0.7714 acc_val: 0.7800 acc_test: 0.7870 time: 1.5361s\n",
      "Epoch: 0139 loss_train: 0.7515 acc_train: 0.8071 acc_val: 0.6320 acc_test: 0.6350 time: 1.5128s\n",
      "Epoch: 0140 loss_train: 0.7159 acc_train: 0.8071 acc_val: 0.6420 acc_test: 0.6510 time: 1.5278s\n",
      "Epoch: 0141 loss_train: 0.7931 acc_train: 0.7929 acc_val: 0.6660 acc_test: 0.6870 time: 1.5193s\n",
      "Epoch: 0142 loss_train: 0.6772 acc_train: 0.8571 acc_val: 0.6280 acc_test: 0.6350 time: 1.5257s\n",
      "Epoch: 0143 loss_train: 0.6334 acc_train: 0.8429 acc_val: 0.6260 acc_test: 0.6330 time: 1.5205s\n",
      "Epoch: 0144 loss_train: 0.6192 acc_train: 0.9000 acc_val: 0.6760 acc_test: 0.6920 time: 1.5039s\n",
      "Epoch: 0145 loss_train: 0.7192 acc_train: 0.8143 acc_val: 0.6940 acc_test: 0.7090 time: 1.5067s\n",
      "Epoch: 0146 loss_train: 0.8980 acc_train: 0.9429 acc_val: 0.6300 acc_test: 0.6400 time: 1.5339s\n",
      "Epoch: 0147 loss_train: 0.6300 acc_train: 0.8000 acc_val: 0.6300 acc_test: 0.6510 time: 1.5111s\n",
      "Epoch: 0148 loss_train: 0.6363 acc_train: 0.8643 acc_val: 0.7840 acc_test: 0.8020 time: 1.5054s\n",
      "Epoch: 0149 loss_train: 0.6654 acc_train: 0.7857 acc_val: 0.6340 acc_test: 0.6550 time: 1.5145s\n",
      "Epoch: 0150 loss_train: 0.7594 acc_train: 0.7571 acc_val: 0.6480 acc_test: 0.6650 time: 1.5294s\n",
      "Epoch: 0151 loss_train: 0.7177 acc_train: 0.7786 acc_val: 0.6340 acc_test: 0.6540 time: 1.4631s\n",
      "Epoch: 0152 loss_train: 0.6553 acc_train: 0.8214 acc_val: 0.6540 acc_test: 0.6630 time: 1.4833s\n",
      "Epoch: 0153 loss_train: 0.5920 acc_train: 0.8429 acc_val: 0.6840 acc_test: 0.7040 time: 1.5493s\n",
      "Epoch: 0154 loss_train: 0.7203 acc_train: 0.8214 acc_val: 0.6520 acc_test: 0.6600 time: 1.5228s\n",
      "Epoch: 0155 loss_train: 0.6547 acc_train: 0.9429 acc_val: 0.6580 acc_test: 0.6710 time: 1.5100s\n",
      "Epoch: 0156 loss_train: 0.5846 acc_train: 0.8714 acc_val: 0.7960 acc_test: 0.7990 time: 1.5832s\n",
      "Epoch: 0157 loss_train: 0.6541 acc_train: 0.8214 acc_val: 0.7360 acc_test: 0.7610 time: 1.5200s\n",
      "Epoch: 0158 loss_train: 0.6126 acc_train: 0.9071 acc_val: 0.6520 acc_test: 0.6720 time: 1.4973s\n",
      "Epoch: 0159 loss_train: 0.6047 acc_train: 0.8500 acc_val: 0.6280 acc_test: 0.6390 time: 1.5033s\n",
      "Epoch: 0160 loss_train: 0.6320 acc_train: 0.8357 acc_val: 0.6240 acc_test: 0.6370 time: 1.4985s\n",
      "Epoch: 0161 loss_train: 0.9300 acc_train: 0.7286 acc_val: 0.6300 acc_test: 0.6330 time: 1.5025s\n",
      "Epoch: 0162 loss_train: 0.7313 acc_train: 0.9714 acc_val: 0.6320 acc_test: 0.6440 time: 1.4949s\n",
      "Epoch: 0163 loss_train: 0.6473 acc_train: 0.8214 acc_val: 0.6300 acc_test: 0.6430 time: 1.4924s\n",
      "Epoch: 0164 loss_train: 0.5882 acc_train: 0.8286 acc_val: 0.6240 acc_test: 0.6380 time: 1.5140s\n",
      "Epoch: 0165 loss_train: 0.6477 acc_train: 0.7857 acc_val: 0.6400 acc_test: 0.6500 time: 1.5125s\n",
      "Epoch: 0166 loss_train: 0.5895 acc_train: 0.8643 acc_val: 0.6340 acc_test: 0.6410 time: 1.4984s\n",
      "Epoch: 0167 loss_train: 0.6092 acc_train: 0.8429 acc_val: 0.6600 acc_test: 0.6650 time: 1.5060s\n",
      "Epoch: 0168 loss_train: 1.0472 acc_train: 0.6429 acc_val: 0.6500 acc_test: 0.6570 time: 1.5103s\n",
      "Epoch: 0169 loss_train: 0.7042 acc_train: 0.7929 acc_val: 0.6380 acc_test: 0.6430 time: 1.4919s\n",
      "Epoch: 0170 loss_train: 0.5667 acc_train: 0.8571 acc_val: 0.6380 acc_test: 0.6480 time: 1.5130s\n",
      "Epoch: 0171 loss_train: 0.6202 acc_train: 0.8571 acc_val: 0.6340 acc_test: 0.6490 time: 1.5366s\n",
      "Epoch: 0172 loss_train: 0.6771 acc_train: 0.9429 acc_val: 0.6620 acc_test: 0.6820 time: 1.5226s\n",
      "Epoch: 0173 loss_train: 0.6649 acc_train: 0.8071 acc_val: 0.6880 acc_test: 0.7090 time: 1.4918s\n",
      "Epoch: 0174 loss_train: 0.5864 acc_train: 0.8714 acc_val: 0.8040 acc_test: 0.8090 time: 1.5366s\n",
      "Epoch: 0175 loss_train: 0.4691 acc_train: 0.9000 acc_val: 0.6420 acc_test: 0.6500 time: 1.5775s\n",
      "Epoch: 0176 loss_train: 0.6115 acc_train: 0.8500 acc_val: 0.6800 acc_test: 0.6990 time: 1.4811s\n",
      "Epoch: 0177 loss_train: 0.5426 acc_train: 0.8500 acc_val: 0.6580 acc_test: 0.6740 time: 1.5524s\n",
      "Epoch: 0178 loss_train: 0.6775 acc_train: 0.9000 acc_val: 0.6560 acc_test: 0.6700 time: 1.4790s\n",
      "Epoch: 0179 loss_train: 0.6493 acc_train: 0.9429 acc_val: 0.7840 acc_test: 0.8010 time: 1.5311s\n",
      "Epoch: 0180 loss_train: 0.6815 acc_train: 0.7929 acc_val: 0.6940 acc_test: 0.7090 time: 1.4044s\n",
      "Epoch: 0181 loss_train: 0.5324 acc_train: 0.8571 acc_val: 0.6380 acc_test: 0.6600 time: 1.5232s\n",
      "Epoch: 0182 loss_train: 0.5988 acc_train: 0.7929 acc_val: 0.7980 acc_test: 0.8170 time: 1.5008s\n",
      "Epoch: 0183 loss_train: 0.5059 acc_train: 0.8643 acc_val: 0.6520 acc_test: 0.6660 time: 1.5102s\n",
      "Epoch: 0184 loss_train: 0.6635 acc_train: 0.8429 acc_val: 0.6420 acc_test: 0.6550 time: 1.4998s\n",
      "Epoch: 0185 loss_train: 0.5109 acc_train: 0.8714 acc_val: 0.6780 acc_test: 0.6890 time: 1.4988s\n",
      "Epoch: 0186 loss_train: 0.6206 acc_train: 0.8857 acc_val: 0.6520 acc_test: 0.6570 time: 1.4997s\n",
      "Epoch: 0187 loss_train: 0.6184 acc_train: 0.8429 acc_val: 0.7920 acc_test: 0.8140 time: 1.5182s\n",
      "Epoch: 0188 loss_train: 0.5435 acc_train: 0.8857 acc_val: 0.6540 acc_test: 0.6610 time: 1.5007s\n",
      "Epoch: 0189 loss_train: 0.5778 acc_train: 0.9357 acc_val: 0.6640 acc_test: 0.6870 time: 1.5611s\n",
      "Epoch: 0190 loss_train: 0.6021 acc_train: 0.8214 acc_val: 0.6200 acc_test: 0.6340 time: 1.5182s\n",
      "Epoch: 0191 loss_train: 0.4974 acc_train: 0.9071 acc_val: 0.6320 acc_test: 0.6500 time: 1.4831s\n",
      "Epoch: 0192 loss_train: 0.4284 acc_train: 0.9500 acc_val: 0.6240 acc_test: 0.6390 time: 1.4936s\n",
      "Epoch: 0193 loss_train: 0.5145 acc_train: 0.9500 acc_val: 0.6300 acc_test: 0.6450 time: 1.5391s\n",
      "Epoch: 0194 loss_train: 0.5218 acc_train: 0.8500 acc_val: 0.6240 acc_test: 0.6420 time: 1.5065s\n",
      "Epoch: 0195 loss_train: 0.6129 acc_train: 0.8357 acc_val: 0.7420 acc_test: 0.7720 time: 1.4960s\n",
      "Epoch: 0196 loss_train: 0.6468 acc_train: 0.8786 acc_val: 0.6440 acc_test: 0.6590 time: 1.5532s\n",
      "Epoch: 0197 loss_train: 0.6196 acc_train: 0.8429 acc_val: 0.6400 acc_test: 0.6610 time: 1.4833s\n",
      "Epoch: 0198 loss_train: 0.4461 acc_train: 0.9000 acc_val: 0.6740 acc_test: 0.6980 time: 1.6276s\n",
      "Epoch: 0199 loss_train: 0.6171 acc_train: 0.8500 acc_val: 0.6660 acc_test: 0.6890 time: 1.4368s\n",
      "Epoch: 0200 loss_train: 0.5026 acc_train: 0.8643 acc_val: 0.8000 acc_test: 0.8150 time: 1.5143s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 314.2293s\n",
      "0.808 0.815\n",
      "0.806 0.805\n",
      "0.804 0.809\n",
      "0.8 0.815\n",
      "0.798 0.817\n",
      "0.796 0.799\n",
      "0.792 0.814\n",
      "0.79 0.794\n",
      "0.788 0.788\n",
      "0.786 0.788\n"
     ]
    }
   ],
   "source": [
    "# Model and optimizer\n",
    "for i in range(2):\n",
    "    print(\"parameters setting here\")\n",
    "    print('features.shape',features.shape) #features.shape torch.Size([2708, 1433])\n",
    "    print(\"labels.max().item()\",labels.max().item()) #labels.max().item() 6 [0,...,6]\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5,hop_num=3) \n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)\n",
    "    \n",
    "    \"\"\"\n",
    "    Optimization Finished!\n",
    "    Total time elapsed: 28.3611s\n",
    "    0.808 0.828\n",
    "    0.806 0.83\n",
    "    0.804 0.827\n",
    "    0.802 0.834\n",
    "    0.8 0.822\n",
    "    0.798 0.825\n",
    "    0.796 0.83\n",
    "    0.794 0.826\n",
    "    0.792 0.822\n",
    "    0.79 0.828\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nclass, dropout):\n",
    "        super(GCN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc3 = GraphConvolution(nhid2, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj): \n",
    "#         x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = F.relu(self.gc1(x, adj))\n",
    "        x1_d = F.dropout(x1, self.dropout, training=self.training)\n",
    "#         combined = torch.cat([feature, x1_d], dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.gc2(x1_d, adj))\n",
    "        x2_d = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = self.gc3(x2_d, adj)\n",
    "        return F.log_softmax(x3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid1=16,\n",
    "            nhid2=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nhid3,nclass, dropout):\n",
    "        super(GCN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc3 = GraphConvolution(nhid2, nhid3,bias=True)\n",
    "        self.gc4 = GraphConvolution(nhid3, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = F.relu(self.gc1(x, adj))\n",
    "        x1_d = F.dropout(x1, self.dropout, training=self.training)\n",
    "#         combined = torch.cat([feature, x1_d], dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.gc2(x1_d, adj))\n",
    "        x2_d = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.gc3(x2_d, adj))\n",
    "        x3_d = F.dropout(x3, self.dropout, training=self.training)\n",
    "        \n",
    "        x4 = self.gc4(x3_d, adj)\n",
    "        return F.log_softmax(x4, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid1=16,\n",
    "            nhid2=16,\n",
    "            nhid3=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nhid3,nhid4,nclass, dropout):\n",
    "        super(GCN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc3 = GraphConvolution(nhid2, nhid3,bias=True)\n",
    "        self.gc4 = GraphConvolution(nhid3, nhid4,bias=True)\n",
    "        self.gc5 = GraphConvolution(nhid4, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = F.relu(self.gc1(x, adj))\n",
    "        x1_d = F.dropout(x1, self.dropout, training=self.training)\n",
    "#         combined = torch.cat([feature, x1_d], dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.gc2(x1_d, adj))\n",
    "        x2_d = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.gc3(x2_d, adj))\n",
    "        x3_d = F.dropout(x3, self.dropout, training=self.training)\n",
    "        \n",
    "        x4 = F.relu(self.gc4(x3_d, adj))\n",
    "        x4_d = F.dropout(x4, self.dropout, training=self.training)\n",
    "        \n",
    "        x5 = self.gc5(x4_d, adj)\n",
    "        return F.log_softmax(x5, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid1=16,\n",
    "            nhid2=16,\n",
    "            nhid3=16,\n",
    "            nhid4=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nhid3,nhid4,nhid5,nclass, dropout):\n",
    "        super(GCN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc3 = GraphConvolution(nhid2, nhid3,bias=True)\n",
    "        self.gc4 = GraphConvolution(nhid3, nhid4,bias=True)\n",
    "        self.gc5 = GraphConvolution(nhid4, nhid5,bias=True)\n",
    "        self.gc6 = GraphConvolution(nhid5, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = F.relu(self.gc1(x, adj))\n",
    "        x1_d = F.dropout(x1, self.dropout, training=self.training)\n",
    "#         combined = torch.cat([feature, x1_d], dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.gc2(x1_d, adj))\n",
    "        x2_d = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.gc3(x2_d, adj))\n",
    "        x3_d = F.dropout(x3, self.dropout, training=self.training)\n",
    "        \n",
    "        x4 = F.relu(self.gc4(x3_d, adj))\n",
    "        x4_d = F.dropout(x4, self.dropout, training=self.training)\n",
    "        \n",
    "        x5 = F.relu(self.gc5(x4_d, adj))\n",
    "        x5_d = F.dropout(x5, self.dropout, training=self.training)\n",
    "        \n",
    "        x6 = self.gc6(x5_d, adj)\n",
    "        return F.log_softmax(x6, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid1=16,\n",
    "            nhid2=16,\n",
    "            nhid3=16,\n",
    "            nhid4=16,\n",
    "            nhid5=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4432a1451976ad171c956c83901d8d528a999525804a07d32d7949c829e79ebf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
