{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solutions:\n",
    "\n",
    "##### propagation asyn.: cnt->concat(features,ngh structure)\n",
    "\n",
    "##### conservation: normalize as the tot in local/global\n",
    "\n",
    "##### propagation dec.: dis&time->dec function\n",
    "\n",
    "##### local structure feature(sample+global vec)\n",
    "\n",
    "kernel methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2129941723.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\LdTenacity\\AppData\\Local\\Temp\\ipykernel_67452\\2129941723.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    def HK():\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# kernel method\n",
    "def PPR():\n",
    "    \n",
    "def HK():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation normalization\n",
    "def interactive_conservation():\n",
    "    # each time the X remains for the same shape, and the changes of the matrix score part means the linux partition ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asynchronous propagation\n",
    "def asynchronous_prop():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local strucuture sample\n",
    "def local_structure_sample():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propagation descent\n",
    "def propgation_heat_descent():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-hop interaction\n",
    "def k_hop_interaction():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from scipy.sparse import csgraph\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def normalize_adj(mx):#D^(-1/2)^T*A*D^(-1/2) preprocess 得到一个领域传播位权矩阵\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    normalize_adj process\n",
    "    adj--original matrix shape (2708, 2708)\n",
    "    adj--degree sum matrix shape (2708, 1)\n",
    "    adj--degree inv matrix shape (2708,)\n",
    "    adj--degree no inf matrix shape (2708,)\n",
    "    adj--degree norm diag matrix shape (2708, 2708)\n",
    "    adj--res shape (2708, 2708)\n",
    "    \"\"\"\n",
    "    ## print(\"normalize_adj process\")\n",
    "    ## print(\"adj--original matrix shape\",mx.shape)\n",
    "    rowsum = np.array(mx.sum(1)) #每行求和，求得每个节点的出度列矩阵\n",
    "    ## print(\"adj--degree sum matrix shape\",rowsum.shape)\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    ## print(\"adj--degree inv matrix shape\",r_inv_sqrt.shape)\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    ## print(\"adj--degree no inf matrix shape\",r_inv_sqrt.shape)\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    ## print(\"adj--degree norm diag matrix shape\",r_mat_inv_sqrt.shape)\n",
    "    resm = mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo()\n",
    "    ## print(\"adj--res shape\",resm.shape)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo() #D^(-1/2)*A*D^(-1/2)^T\n",
    "\n",
    "def normalize(mx):#D^-(1)*A\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    normalize process\n",
    "    mx--original matrix shape (2708, 1433)\n",
    "    mx--degree sum matrix shape (2708, 1)\n",
    "    mx--degree inv matrix shape (2708,)\n",
    "    mx--degree no inf matrix shape (2708,)\n",
    "    mx--degree norm diag matrix shape (2708, 2708)\n",
    "    mx--res shape (2708, 1433)\n",
    "    \"\"\"  \n",
    "    ## print(\"normalize process\")\n",
    "    ## print(\"mx--original matrix shape\",mx.shape)\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    ## print(\"mx--degree sum matrix shape\",rowsum.shape)\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    ## print(\"mx--degree inv matrix shape\",r_inv.shape)\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    ## print(\"mx--degree no inf matrix shape\",r_inv.shape)\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    ## print(\"mx--degree norm diag matrix shape\",r_mat_inv.shape)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    ## print(\"mx--res shape\",mx.shape)\n",
    "    return mx\n",
    "\n",
    "def laplacian(mx, norm):\n",
    "    \"\"\"Laplacian-normalize sparse matrix\"\"\"\n",
    "    assert (all (len(row) == len(mx) for row in mx)), \"Input should be a square matrix\"\n",
    "\n",
    "    return csgraph.laplacian(adj, normed = norm)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def load_data(path=\"../data\", dataset=\"cora\"):\n",
    "    \"\"\"\n",
    "    ind.[:dataset].x     => the feature vectors of the training instances (scipy.sparse.csr.csr_matrix)\n",
    "    ind.[:dataset].y     => the one-hot labels of the labeled training instances (numpy.ndarray)\n",
    "    ind.[:dataset].allx  => the feature vectors of both labeled and unlabeled training instances (csr_matrix)\n",
    "    ind.[:dataset].ally  => the labels for instances in ind.dataset_str.allx (numpy.ndarray)\n",
    "    ind.[:dataset].graph => the dict in the format {index: [index of neighbor nodes]} (collections.defaultdict)\n",
    "    ind.[:dataset].tx => the feature vectors of the test instances (scipy.sparse.csr.csr_matrix)\n",
    "    ind.[:dataset].ty => the one-hot labels of the test instances (numpy.ndarray)\n",
    "    ind.[:dataset].test.index => indices of test instances in graph, for the inductive setting\n",
    "    \"\"\"\n",
    "    print(\"\\n[STEP 1]: Upload {} dataset.\".format(dataset))\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        with open(\"{}/ind.{}.{}\".format(path, dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    \n",
    "    ## print(\"x.shape=\",x.shape)\n",
    "    ## print(\"y.shape=\",y.shape)\n",
    "    ## print(\"tx.shape=\",tx.shape)\n",
    "    ## print(\"ty.shape=\",ty.shape)\n",
    "    ## print(\"allx=\",allx.shape)\n",
    "    ## print(\"ally=\",ally.shape)\n",
    "    ## print(\"len(graph)=\",len(graph))\n",
    "    \"\"\"\n",
    "    x.shape= (140, 1433)\n",
    "    y.shape= (140, 7)\n",
    "    tx.shape= (1000, 1433)\n",
    "    ty.shape= (1000, 7)\n",
    "    allx= (1708, 1433)\n",
    "    ally= (1708, 7)\n",
    "    len(graph)= 2708\n",
    "    \"\"\"\n",
    "    \n",
    "    ## print(\"x=\",x)\n",
    "    ## print(\"y=\",y)\n",
    "    ## print(\"tx=\",tx)\n",
    "    ## print(\"ty=\",ty)\n",
    "    ## print(\"allx=\",allx)\n",
    "    ## print(\"ally=\",ally)\n",
    "    ## print(\"graph=\",graph)\n",
    "\n",
    "    test_idx_reorder = parse_index_file(\"{}/ind.{}.test.index\".format(path, dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        #Citeseer dataset contains some isolated nodes in the graph\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "    print(\"| # of nodes : {}\".format(adj.shape[0]))\n",
    "    print(\"| # of edges : {}\".format(adj.sum().sum()/2))\n",
    "\n",
    "    features = normalize(features) #将feature按照度归一化\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0])) #将A按照两个边端点度归一化\n",
    "    \n",
    "    ###### \n",
    "    ### AF MATRIX\n",
    "    ######\n",
    "    #####################\n",
    "    ########################## adj = AF_matrix(adj, features, 3)\n",
    "    #####################\n",
    "    ## AF = AF_matrix(adj, features, 3)\n",
    "    \n",
    "    print(\"| # of features : {}\".format(features.shape[1]))\n",
    "    print(\"| # of clases   : {}\".format(ally.shape[1]))\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    sparse_mx = adj.tocoo().astype(np.float32)\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        save_label = np.where(labels)[1]\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "    idx_test = test_idx_range.tolist()\n",
    "\n",
    "    print(\"| # of train set : {}\".format(len(idx_train)))\n",
    "    print(\"| # of val set   : {}\".format(len(idx_val)))\n",
    "    print(\"| # of test set  : {}\".format(len(idx_test)))\n",
    "\n",
    "    idx_train, idx_val, idx_test = list(map(lambda x: torch.LongTensor(x), [idx_train, idx_val, idx_test]))\n",
    "\n",
    "    def missing_elements(L):\n",
    "        start, end = L[0], L[-1]\n",
    "        return sorted(set(range(start, end+1)).difference(L))\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        L = np.sort(idx_test)\n",
    "        missing = missing_elements(L)\n",
    "\n",
    "        for element in missing:\n",
    "            save_label = np.insert(save_label, element, 0)\n",
    "\n",
    "        labels = torch.LongTensor(save_label)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj_torch(torch_mx):#D^(-1/2)^T*A*D^(-1/2) preprocess 得到一个领域传播位权矩阵\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    normalize_adj process\n",
    "    adj--original matrix shape (2708, 2708)\n",
    "    adj--degree sum matrix shape (2708, 1)\n",
    "    adj--degree inv matrix shape (2708,)\n",
    "    adj--degree no inf matrix shape (2708,)\n",
    "    adj--degree norm diag matrix shape (2708, 2708)\n",
    "    adj--res shape (2708, 2708)\n",
    "    \"\"\"\n",
    "    ## print(\"normalize_adj process\")\n",
    "    ## print(\"adj--original matrix shape\",mx.shape)\n",
    "    \"\"\"\n",
    "    rowsum = np.array(torch_mx.sum(1)) #每行求和，求得每个节点的出度列矩阵\n",
    "    ## print(\"adj--degree sum matrix shape\",rowsum.shape)\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    ## print(\"adj--degree inv matrix shape\",r_inv_sqrt.shape)\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    ## print(\"adj--degree no inf matrix shape\",r_inv_sqrt.shape)\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    ## print(\"adj--degree norm diag matrix shape\",r_mat_inv_sqrt.shape)\n",
    "    resm = np.array(torch_mx).dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo()\n",
    "    ## print(\"adj--res shape\",resm.shape)\n",
    "    resm = torch.from_numpy(resm)\n",
    "    \"\"\"\n",
    "    rowsum = torch_mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum,-0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)]=0.\n",
    "    r_mat_inv_sqrt = torch.diag_embed(r_inv_sqrt)\n",
    "    resm = torch.mm(r_mat_inv_sqrt.T,torch_mx)\n",
    "    resm = torch.mm(resm,r_mat_inv_sqrt)\n",
    "    ## resm.requires_grad = False only can change on the leaf node\n",
    "    return resm#mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo() #D^(-1/2)*A*D^(-1/2)^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AF_matrix(adj, feature, hop_num):#D^(-1/2)^T*(AF)*(AF)^T*D^(-1/2)\n",
    "    #L_AF self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "    \n",
    "    #PVM\n",
    "    \n",
    "    # adj 2708, feature 1433\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"\n",
    "    For adjancy matrix\n",
    "    normalize_adj process\n",
    "    adj--original matrix shape (2708, 2708)\n",
    "    adj--degree sum matrix shape (2708, 1)\n",
    "    adj--degree inv matrix shape (2708,)\n",
    "    adj--degree no inf matrix shape (2708,)\n",
    "    adj--degree norm diag matrix shape (2708, 2708)\n",
    "    adj--res shape (2708, 2708)\n",
    "    \n",
    "    For feature matrix\n",
    "    normalize process\n",
    "    mx--original matrix shape (2708, 1433)\n",
    "    mx--degree sum matrix shape (2708, 1)\n",
    "    mx--degree inv matrix shape (2708,)\n",
    "    mx--degree no inf matrix shape (2708,)\n",
    "    mx--degree norm diag matrix shape (2708, 2708)\n",
    "    mx--res shape (2708, 1433)\n",
    "    \"\"\"\n",
    "    A_curhop = adj\n",
    "    resm = adj\n",
    "    dvec = torch.ones(adj.shape[0],1)#torch.ones(3*hop_num,1)\n",
    "    L_AF = Parameter(torch.FloatTensor(1,3*hop_num))\n",
    "    torch.nn.init.normal_(L_AF)\n",
    "    \"\"\"\n",
    "    1 0 1\n",
    "    1 0 1\n",
    "    1 0 1\n",
    "    对应一个对应AF矩阵的值\n",
    "    \"\"\"\n",
    "    # PVM = np.array(torch.FloatTensor(3*hop_num,(adj.shape[1]+adj.shape[1]+feature.shape[1])*hop_num))\n",
    "    PVM = torch.ones(3*hop_num,adj.shape[1])*L_AF[0][0]\n",
    "    print(\"PVM.shape\",PVM.shape)\n",
    "    for i in range(1,3*hop_num):\n",
    "        print(\"i'th PVM expansion\")\n",
    "        if((i%3) == 2) :\n",
    "            PVM = torch.cat((PVM,torch.ones(3*hop_num,feature.shape[1])*L_AF[0][i]),1)\n",
    "            print(PVM.shape)\n",
    "        else:\n",
    "            PVM = torch.cat((PVM,torch.ones(3*hop_num,adj.shape[1])*L_AF[0][i]),1)\n",
    "            print(PVM.shape)\n",
    "    \"\"\"\n",
    "    PVM = None\n",
    "    for i in range(0,3*hop_num):\n",
    "        if(i%3 == 0):\n",
    "            PVM.concat((PVM,np.ones(3*hop_num,adj.shape[1])*L_AF[i]))\n",
    "        else:\n",
    "            PVM.concat((PVM,np.ones(3*hop_num,feature.shape[1])*L_AF[i]))\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0,hop_num):\n",
    "        print(\"{}'th hop normalize_adj process\".format(i))\n",
    "        A_curhop_D = np.array(A_curhop.sum(1)).flatten() #每行求和，求得每个节点的出度列矩阵 A_curhop_D var mean\n",
    "        ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)\n",
    "        A_curhop_D = sp.diags(A_curhop_D) #var mean\n",
    "        ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)\n",
    "        # 进行A^k的k步传播，选择Feature矩阵的非0位置\n",
    "        feature_curhop = A_curhop.dot(feature)#feature[A_cur_hop where not 0] #var mean\n",
    "        ##print(\"feature_curhop.shape\",feature_curhop.shape)\n",
    "        #degree_hop = (A_cur_hop[A_cur_hop!=0]=A_curhop_D) #var mean\n",
    "        \"\"\"\n",
    "        print(\"the adj matrix\",A_curhop)\n",
    "        print(\"the adj matrix.shape\",A_curhop.shape)\n",
    "        print(\"the degree matrix\",A_curhop_D)\n",
    "        print(\"the degree matrix.shape\",A_curhop_D.shape)\n",
    "        print(\"the adj feature matrix\",feature_curhop) #A^hopnum * feature\n",
    "        print(\"the adj feature matrix.shape\",feature_curhop.shape)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        adj_rowsum = np.array(A_cur_hop.sum(1)) #每行求和，求得每个节点的出度列矩阵 A_curhop_D\n",
    "        print(\"adj--degree sum matrix shape\",adj_rowsum.shape)\n",
    "        adj_r_inv_sqrt = np.power(adj_rowsum, -0.5).flatten()\n",
    "        print(\"adj--degree inv matrix shape\",adj_r_inv_sqrt.shape)#A_curhop_D^(-1/2)\n",
    "        adj_r_inv_sqrt[np.isinf(adj_r_inv_sqrt)] = 0.\n",
    "        print(\"adj--degree no inf matrix shape\",adj_r_inv_sqrt.shape)\n",
    "        adj_r_mat_inv_sqrt = sp.diags(adj_r_inv_sqrt) #A_D^(-1/2)\n",
    "        print(\"adj--degree norm diag matrix shape\",adj_r_mat_inv_sqrt.shape) #A_curhop_D^(-1/2)\n",
    "        #norm_adj_resm = adj.dot(adj_r_mat_inv_sqrt).transpose().dot(adj_r_mat_inv_sqrt).tocoo()\n",
    "        #print(\"adj--res shape\",resm.shape)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(\"concatenate members shape\")\n",
    "        print(\"A_curhop.shape=\",A_curhop.shape)\n",
    "        print(\"A_curhop_D.shape\",A_curhop_D.shape)\n",
    "        \"\"\"\n",
    "        if(i !=0):\n",
    "            resm = sp.hstack((A_curhop, resm))\n",
    "        ##print(\"first time concat.shape\",resm.shape)\n",
    "        ##print(\"feature_curhop.shape\",feature_curhop.shape)\n",
    "        resm = sp.hstack((A_curhop_D, resm))\n",
    "        resm = sp.hstack((feature_curhop, resm))\n",
    "        ##print(\"the resm here is\")\n",
    "        ##print(resm.shape)\n",
    "        A_curhop  =A_curhop.dot(adj)\n",
    "    print(\"dvec shape\")\n",
    "    print(dvec.shape) #(9, 1)\n",
    "    print(\"L_AF shape\")\n",
    "    print(L_AF.shape) # torch.Size([1, 9])\n",
    "    print(\"PVM shape\")\n",
    "    print(PVM.shape) # (9, 20547) 1|0|0|1|1|0\n",
    "    # (9,1)*(1,9)*(9,20547)\n",
    "    \"\"\"\n",
    "    L_AF shape\n",
    "    torch.Size([1, 9])\n",
    "    PVM shape\n",
    "    (9, 20547)\n",
    "    | # of features : 1433\n",
    "    | # of clases   : 7\n",
    "    | # of train set : 140\n",
    "    | # of val set   : 500\n",
    "    | # of test set  : 1000\n",
    "    \"\"\"\n",
    "    \n",
    "    #resm #= sp.coo_matrix(a_matrix)#torch.from_numpy(resm) #tmp_coo=sp.coo_matrix(a_matrix)\n",
    "    resm = resm\n",
    "    values=resm.data\n",
    "    indices=np.vstack((resm.row,resm.col))\n",
    "    i=torch.LongTensor(indices)\n",
    "    v=torch.LongTensor(values)\n",
    "    resm =torch.sparse_coo_tensor(i,v,resm.shape)\n",
    "    resm = resm.to_dense()\n",
    "    \n",
    "    mux_res = torch.mm(torch.mm(dvec,L_AF),PVM) #resm*L_AF*PVM\n",
    "    print(\"mux_res result size\",mux_res.shape)\n",
    "    #print(\"preprocess matrix\",mux_res)\n",
    "    \n",
    "    print(\"the actual A-F matrix shape\",resm.shape)\n",
    "    \n",
    "    weighted_resm = torch.mul(mux_res,resm)#mux_res * resm\n",
    "    \n",
    "    mm_res = torch.mm(weighted_resm,weighted_resm.T)\n",
    "    print(\"result multiplication shape\",mm_res.shape)\n",
    "    \n",
    "    return mm_res #resm,L_AF,PVM #D^(-1/2)*A*D^(-1/2)^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model,record):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj, features)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward(retain_graph=True)  \n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    output = model(features, adj, features)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'acc_test: {:.4f}'.format(acc_test.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    record[acc_val.item()] = acc_test.item() #record: acc_val->acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, hop_num,bias=True):\n",
    "        ## print(\"init_state of GCN\")\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        ######此处定义了网路的parameter例如weight和bias 两者在init里面定义为parameter 在self保存为net的结构\n",
    "        self.hop_num = hop_num\n",
    "        self.L_AF = Parameter(torch.FloatTensor(1,3*hop_num))\n",
    "        factor = 1\n",
    "        \"\"\"\n",
    "        for i in range(0,hop_num):\n",
    "            self.L_AF[3*i] = factor\n",
    "            self.L_AF[3*i+1] = factor*0.1\n",
    "            self.L_AF[3*i+1] = factor*0.001\n",
    "            factor *=0.1\n",
    "        \"\"\"\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features)) #线性Linear层负责convolutional变换 \n",
    "        #是两个维度的转换 为1433,16 16.7\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features)) #bias大小为输出层上的偏执 可以为16 7\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        ## print(\"reset_state of GCN\")\n",
    "        ### reset linear layer\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        ## print(\"stdv=\",stdv)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj, feature):\n",
    "        adj = self.AF_matrix(adj, feature, self.hop_num)\n",
    "        \"\"\"\n",
    "        print(\"forward_state of GCN\")\n",
    "        print(\"input.shape=\",input.shape) #input.shape= torch.Size([2708, 1433]) 0/1\n",
    "        print(\"input=\",input)\n",
    "        print(\"adj.shape=\",adj.shape) #adj.shape= torch.Size([2708, 2708]) float\n",
    "        print(\"adj=\",adj)\n",
    "        \"\"\"\n",
    "        support = torch.mm(input, self.weight) #torch.mm矩阵乘法 线性层的个数为 1433*16 (in_features, out_features)\n",
    "        \"\"\"\n",
    "        print(\"support.shape=\",support.shape) #support.shape= torch.Size([2708, 16]) float\n",
    "        print(\"support=\",support)\n",
    "        \"\"\"\n",
    "        output = torch.spmm(adj, support) #A*F*L spmm稀疏矩阵乘法\n",
    "        \"\"\"\n",
    "        print(\"output.shape=\",output.shape)#output.shape= torch.Size([2708, 16]) float\n",
    "        print(\"output=\",output)\n",
    "        print(\"bias.shape=\",self.bias.shape) #bias.shape= torch.Size([16]) float\n",
    "        print(\"bias=\",self.bias)\n",
    "        \"\"\"\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        print(\"repr_state of GCN\")\n",
    "        print(self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')')\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    def AF_matrix(self, adj, feature, hop_num):#D^(-1/2)^T*(AF)*(AF)^T*D^(-1/2)\n",
    "        #L_AF self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "        #PVM\n",
    "\n",
    "        # adj 2708, feature 1433\n",
    "        \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "        \"\"\"\n",
    "        For adjancy matrix\n",
    "        normalize_adj process\n",
    "        adj--original matrix shape (2708, 2708)\n",
    "        adj--degree sum matrix shape (2708, 1)\n",
    "        adj--degree inv matrix shape (2708,)\n",
    "        adj--degree no inf matrix shape (2708,)\n",
    "        adj--degree norm diag matrix shape (2708, 2708)\n",
    "        adj--res shape (2708, 2708)\n",
    "\n",
    "        For feature matrix\n",
    "        normalize process\n",
    "        mx--original matrix shape (2708, 1433)\n",
    "        mx--degree sum matrix shape (2708, 1)\n",
    "        mx--degree inv matrix shape (2708,)\n",
    "        mx--degree no inf matrix shape (2708,)\n",
    "        mx--degree norm diag matrix shape (2708, 2708)\n",
    "        mx--res shape (2708, 1433)\n",
    "        \"\"\"\n",
    "        adj = normalize_adj_torch(adj)\n",
    "        A_curhop = adj\n",
    "        resm = adj\n",
    "        dvec = torch.ones(adj.shape[0],1)#torch.ones(3*hop_num,1)\n",
    "        #L_AF = Parameter(torch.FloatTensor(1,3*hop_num))\n",
    "        torch.nn.init.normal_(self.L_AF)\n",
    "        \"\"\"\n",
    "        1 0 1\n",
    "        1 0 1\n",
    "        1 0 1\n",
    "        对应一个对应AF矩阵的值\n",
    "        \"\"\"\n",
    "        # PVM = np.array(torch.FloatTensor(3*hop_num,(adj.shape[1]+adj.shape[1]+feature.shape[1])*hop_num))\n",
    "        PVM = torch.ones(3*hop_num,adj.shape[1])*self.L_AF[0][0]\n",
    "        factor = 1.0\n",
    "        ## print(\"PVM.shape\",PVM.shape)\n",
    "        for i in range(1,3*hop_num):\n",
    "            factor *= 0.1\n",
    "            ## print(\"i'th PVM expansion\")\n",
    "            if((i%3) == 2) :\n",
    "                PVM = torch.cat((PVM,torch.ones(3*hop_num,feature.shape[1])*self.L_AF[0][i]),1)\n",
    "                ## print(PVM.shape)\n",
    "            else:\n",
    "                PVM = torch.cat((PVM,torch.ones(3*hop_num,adj.shape[1])*self.L_AF[0][i]),1)\n",
    "                ## print(PVM.shape)\n",
    "        \"\"\"\n",
    "        PVM = None\n",
    "        for i in range(0,3*hop_num):\n",
    "            if(i%3 == 0):\n",
    "                PVM.concat((PVM,np.ones(3*hop_num,adj.shape[1])*L_AF[i]))\n",
    "            else:\n",
    "                PVM.concat((PVM,np.ones(3*hop_num,feature.shape[1])*L_AF[i]))\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(0,hop_num):\n",
    "            ## print(\"{}'th hop normalize_adj process\".format(i))\n",
    "            A_curhop_D = A_curhop.sum(1).flatten() #每行求和，求得每个节点的出度列矩阵 A_curhop_D var mean\n",
    "            ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)\n",
    "            A_curhop_D = A_curhop_D * torch.eye(A_curhop_D.shape[0]) #var mean\n",
    "            ##print(\"A_curhop_D.shape:\",A_curhop_D.shape)torch.eye(A_curhop_D.shape[1])\n",
    "            # 进行A^k的k步传播，选择Feature矩阵的非0位置\n",
    "            ## print(\"A_curhop_D.shape\",A_curhop_D.shape)\n",
    "            ## print(\"feature.shape\",feature.shape)\n",
    "            feature_curhop = torch.mm(A_curhop,feature)\n",
    "            #feature_curhop = A_curhop.dot(feature)#feature[A_cur_hop where not 0] #var mean\n",
    "            ##print(\"feature_curhop.shape\",feature_curhop.shape)\n",
    "            #degree_hop = (A_cur_hop[A_cur_hop!=0]=A_curhop_D) #var mean\n",
    "            \"\"\"\n",
    "            print(\"the adj matrix\",A_curhop)\n",
    "            print(\"the adj matrix.shape\",A_curhop.shape)\n",
    "            print(\"the degree matrix\",A_curhop_D)\n",
    "            print(\"the degree matrix.shape\",A_curhop_D.shape)\n",
    "            print(\"the adj feature matrix\",feature_curhop) #A^hopnum * feature\n",
    "            print(\"the adj feature matrix.shape\",feature_curhop.shape)\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            adj_rowsum = np.array(A_cur_hop.sum(1)) #每行求和，求得每个节点的出度列矩阵 A_curhop_D\n",
    "            print(\"adj--degree sum matrix shape\",adj_rowsum.shape)\n",
    "            adj_r_inv_sqrt = np.power(adj_rowsum, -0.5).flatten()\n",
    "            print(\"adj--degree inv matrix shape\",adj_r_inv_sqrt.shape)#A_curhop_D^(-1/2)\n",
    "            adj_r_inv_sqrt[np.isinf(adj_r_inv_sqrt)] = 0.\n",
    "            print(\"adj--degree no inf matrix shape\",adj_r_inv_sqrt.shape)\n",
    "            adj_r_mat_inv_sqrt = sp.diags(adj_r_inv_sqrt) #A_D^(-1/2)\n",
    "            print(\"adj--degree norm diag matrix shape\",adj_r_mat_inv_sqrt.shape) #A_curhop_D^(-1/2)\n",
    "            #norm_adj_resm = adj.dot(adj_r_mat_inv_sqrt).transpose().dot(adj_r_mat_inv_sqrt).tocoo()\n",
    "            #print(\"adj--res shape\",resm.shape)\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            print(\"concatenate members shape\")\n",
    "            print(\"A_curhop.shape=\",A_curhop.shape)\n",
    "            print(\"A_curhop_D.shape\",A_curhop_D.shape)\n",
    "            \"\"\"\n",
    "            if(i !=0):\n",
    "                #resm = sp.hstack((A_curhop, resm))\n",
    "                resm = torch.cat((A_curhop,resm), 1)\n",
    "            ##print(\"first time concat.shape\",resm.shape)\n",
    "            ##print(\"feature_curhop.shape\",feature_curhop.shape)\n",
    "            #resm = sp.hstack((A_curhop_D, resm))\n",
    "            resm = torch.cat((A_curhop_D, resm),1)\n",
    "            #resm = sp.hstack((feature_curhop, resm))\n",
    "            resm = torch.cat((feature_curhop, resm),1)\n",
    "            ##print(\"the resm here is\")\n",
    "            ##print(resm.shape)\n",
    "            #A_curhop  =A_curhop.dot(adj)\n",
    "            A_curhop = torch.mm(A_curhop,adj)\n",
    "            A_curhop = normalize_adj_torch(A_curhop)\n",
    "        \"\"\"\n",
    "        print(\"dvec shape\")\n",
    "        print(dvec.shape) #(9, 1)\n",
    "        print(\"self.L_AF shape\")\n",
    "        print(self.L_AF.shape) # torch.Size([1, 9])\n",
    "        print(\"PVM shape\")\n",
    "        print(PVM.shape) # (9, 20547) 1|0|0|1|1|0\n",
    "        # (9,1)*(1,9)*(9,20547)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        L_AF shape\n",
    "        torch.Size([1, 9])\n",
    "        PVM shape\n",
    "        (9, 20547)\n",
    "        | # of features : 1433\n",
    "        | # of clases   : 7\n",
    "        | # of train set : 140\n",
    "        | # of val set   : 500\n",
    "        | # of test set  : 1000\n",
    "        \"\"\"\n",
    "\n",
    "        #resm #= sp.coo_matrix(a_matrix)#torch.from_numpy(resm) #tmp_coo=sp.coo_matrix(a_matrix)\n",
    "        \"\"\"\n",
    "        resm = resm\n",
    "        values=resm.data\n",
    "        indices=np.vstack((resm.row,resm.col))\n",
    "        i=torch.LongTensor(indices)\n",
    "        v=torch.LongTensor(values)\n",
    "        resm =torch.sparse_coo_tensor(i,v,resm.shape)\n",
    "        resm = resm.to_dense()\n",
    "        \"\"\"\n",
    "\n",
    "        mux_res = torch.mm(torch.mm(dvec,self.L_AF),PVM) #resm*L_AF*PVM\n",
    "        ## print(\"mux_res result size\",mux_res.shape)\n",
    "        #print(\"preprocess matrix\",mux_res)\n",
    "\n",
    "        ## print(\"the actual A-F matrix shape\",resm.shape)\n",
    "\n",
    "        weighted_resm = torch.mul(mux_res,resm)#mux_res * resm\n",
    "\n",
    "        mm_res = torch.mm(weighted_resm,weighted_resm.T)\n",
    "        ## print(\"result multiplication shape\",mm_res.shape)\n",
    "        # mm_res.requires_grad=False\n",
    "\n",
    "        return mm_res #resm,L_AF,PVM #D^(-1/2)*A*D^(-1/2)^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1]: Upload cora dataset.\n",
      "| # of nodes : 2708\n",
      "| # of edges : 5278.0\n",
      "| # of features : 1433\n",
      "| # of clases   : 7\n",
      "| # of train set : 140\n",
      "| # of val set   : 500\n",
      "| # of test set  : 1000\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正常2层的GCN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout,hop_num):\n",
    "        ## print(\"init_state of 2-layer GCN\")\n",
    "    \n",
    "        super(GCN1, self).__init__() #inherent a GCN model\n",
    "        \n",
    "        \n",
    "        #两层graph convolution\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid,bias=True,hop_num=3)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass,bias=True,hop_num=3)\n",
    "        self.dropout = dropout\n",
    "        self.hop_num = hop_num\n",
    "        \n",
    "        ## print(\"gc1=\",self.gc1)\n",
    "        ## print(\"gc2=\",self.gc2)\n",
    "        ## print(\"dropout=\",self.dropout) #0.5\n",
    "\n",
    "    def forward(self, x, adj,features):\n",
    "        ## print(\"forward_state of 2-layer GCN\")\n",
    "        x_d = F.dropout(x, self.dropout, training=self.training) #对于feature进行drop_out\n",
    "        \"\"\"\n",
    "        print(\"x_d.shape=\",x_d.shape) #x_d.shape= torch.Size([2708, 1433])\n",
    "        print(\"x_d=\",x_d)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        x_d= tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        ...,\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
    "        \"\"\"\n",
    "        ### layer 1 processed result\n",
    "        lrs1 = self.gc1(x_d, adj, features) \n",
    "        #传入的参数有上一轮的一阶邻居聚合信息的映射 x_d, adj \n",
    "        #for new network here should be changed to AF^T*AF\n",
    "        \"\"\"\n",
    "        print(\"layer1 res.shape\",lrs1.shape) #layer1 res.shape torch.Size([2708, 16])\n",
    "        print(\"layer1 res\",lrs1) # AXW^0 #每次提取周围邻居的信息 A为边信息两边度的权重积 并aggregate到自己的信息来 对于1433维度信息做一个统一的线性层映射\n",
    "        \"\"\"\n",
    "        ########################\n",
    "        #我们现在的工作就是要把AX这个仅仅考虑结构的propagte加一个考虑feature的Propagate，\n",
    "        #还有加一个异步的propagate，还有加一个考虑时间和距离衰减的propagate，同时在一定时间后给一定范围内的节点进行Normalization\n",
    "        ########################\n",
    "        x = F.relu(self.gc1(x_d, adj, features)) ################## i-j 已用feature计算\n",
    "        # X=relu(AXW^0) 这里考虑AXW^0 A[node_num,node_num]*X[node_num,feature_num]*W[feature_num,convolved_dim]\n",
    "        # X=[node_num,pre_convolved_dim]\n",
    "        # AXW^i=A[node_num,node_num]*X[node_num,pre_convolved_dim]*L[pre_convolved_dim,latter_convolved_dim]\n",
    "        #A-F = [node_num,LongAF]\n",
    "        # Org.A = (i,j)1-hopAttentioned-A\n",
    "        # A-F^T*A-F = (i,j)multi-hopAttentioned-A\n",
    "        \"\"\"\n",
    "        print(\"after relu GCN1 x.shape=\",x.shape) #after relu GCN1 x.shape= torch.Size([2708, 16])\n",
    "        print(\"after relu GCN1 x=\",x)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        after relu GCN1 x= tensor([[0.0000, 0.0000, 0.1960,  ..., 0.1392, 0.0000, 0.0000],\n",
    "        [0.0256, 0.0000, 0.1576,  ..., 0.2085, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.1568,  ..., 0.1526, 0.0000, 0.0000],\n",
    "        ...,\n",
    "        [0.0000, 0.0000, 0.0998,  ..., 0.1981, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.1614,  ..., 0.1271, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.1632,  ..., 0.1248, 0.0000, 0.0000]],\n",
    "       grad_fn=<ReluBackward0>)\n",
    "        \"\"\"\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \"\"\"\n",
    "        print(\"after dropout x.shape=\",x.shape)\n",
    "        print(\"after dropout x=\",x)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        after dropout x= tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.3152,  ..., 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.3136,  ..., 0.3053, 0.0000, 0.0000],\n",
    "        ...,\n",
    "        [0.0000, 0.0000, 0.1997,  ..., 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.3229,  ..., 0.2542, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.3264,  ..., 0.2497, 0.0000, 0.0000]],\n",
    "       grad_fn=<MulBackward0>)\n",
    "        \"\"\"\n",
    "        x = self.gc2(x, adj, features) #16->7 AXW^1\n",
    "        \"\"\"\n",
    "        print(\"GCN2 x.shape=\",x.shape) #GCN2 x.shape= torch.Size([2708, 7])\n",
    "        print(\"GCN2 x=\",x)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        GCN2 x= tensor([[-0.1629,  0.0549,  0.0964,  ...,  0.2143,  0.2979,  0.0673],\n",
    "        [-0.1236,  0.0818,  0.0692,  ...,  0.2229,  0.3301,  0.0718],\n",
    "        [-0.1413,  0.0453,  0.0838,  ...,  0.2303,  0.3301,  0.0501],\n",
    "        ...,\n",
    "        [-0.1111,  0.0988,  0.1083,  ...,  0.2313,  0.3120,  0.1059],\n",
    "        [-0.0725,  0.1234,  0.0693,  ...,  0.2215,  0.3570,  0.1172],\n",
    "        [-0.0930,  0.1279,  0.0991,  ...,  0.2040,  0.3516,  0.1457]],\n",
    "       grad_fn=<AddBackward0>)\n",
    "        \"\"\"\n",
    "        res = F.log_softmax(x, dim=1) #对于结果进行归一化 softmax(AXW^1)\n",
    "        \"\"\"\n",
    "        print(\"res.shape=\",res.shape) #res.shape= torch.Size([2708, 7])\n",
    "        print(\"res=\",res)\n",
    "        \"\"\"\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n",
      "torch.Size([1433, 16])\n",
      "torch.Size([16])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([16, 7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5,hop_num=3) \n",
    "for item in model1.parameters():\n",
    "        print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters setting here\n",
      "features.shape torch.Size([2708, 1433])\n",
      "labels.max().item() 6\n",
      "Epoch: 0001 loss_train: 1142.4227 acc_train: 0.1429 acc_val: 0.1220 acc_test: 0.1300 time: 45.3699s\n",
      "Epoch: 0002 loss_train: 76.0173 acc_train: 0.1357 acc_val: 0.1220 acc_test: 0.1300 time: 54.4854s\n",
      "Epoch: 0003 loss_train: 83.2766 acc_train: 0.2071 acc_val: 0.1500 acc_test: 0.1430 time: 61.6807s\n"
     ]
    }
   ],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    print(\"parameters setting here\")\n",
    "    print('features.shape',features.shape) #features.shape torch.Size([2708, 1433])\n",
    "    print(\"labels.max().item()\",labels.max().item()) #labels.max().item() 6 [0,...,6]\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5,hop_num=3) \n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)\n",
    "    \n",
    "    \"\"\"\n",
    "    Optimization Finished!\n",
    "    Total time elapsed: 28.3611s\n",
    "    0.808 0.828\n",
    "    0.806 0.83\n",
    "    0.804 0.827\n",
    "    0.802 0.834\n",
    "    0.8 0.822\n",
    "    0.798 0.825\n",
    "    0.796 0.83\n",
    "    0.794 0.826\n",
    "    0.792 0.822\n",
    "    0.79 0.828\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nclass, dropout):\n",
    "        super(GCN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc3 = GraphConvolution(nhid2, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj): \n",
    "#         x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = F.relu(self.gc1(x, adj))\n",
    "        x1_d = F.dropout(x1, self.dropout, training=self.training)\n",
    "#         combined = torch.cat([feature, x1_d], dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.gc2(x1_d, adj))\n",
    "        x2_d = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = self.gc3(x2_d, adj)\n",
    "        return F.log_softmax(x3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid1=16,\n",
    "            nhid2=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nhid3,nclass, dropout):\n",
    "        super(GCN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc3 = GraphConvolution(nhid2, nhid3,bias=True)\n",
    "        self.gc4 = GraphConvolution(nhid3, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = F.relu(self.gc1(x, adj))\n",
    "        x1_d = F.dropout(x1, self.dropout, training=self.training)\n",
    "#         combined = torch.cat([feature, x1_d], dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.gc2(x1_d, adj))\n",
    "        x2_d = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.gc3(x2_d, adj))\n",
    "        x3_d = F.dropout(x3, self.dropout, training=self.training)\n",
    "        \n",
    "        x4 = self.gc4(x3_d, adj)\n",
    "        return F.log_softmax(x4, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid1=16,\n",
    "            nhid2=16,\n",
    "            nhid3=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nhid3,nhid4,nclass, dropout):\n",
    "        super(GCN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc3 = GraphConvolution(nhid2, nhid3,bias=True)\n",
    "        self.gc4 = GraphConvolution(nhid3, nhid4,bias=True)\n",
    "        self.gc5 = GraphConvolution(nhid4, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = F.relu(self.gc1(x, adj))\n",
    "        x1_d = F.dropout(x1, self.dropout, training=self.training)\n",
    "#         combined = torch.cat([feature, x1_d], dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.gc2(x1_d, adj))\n",
    "        x2_d = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.gc3(x2_d, adj))\n",
    "        x3_d = F.dropout(x3, self.dropout, training=self.training)\n",
    "        \n",
    "        x4 = F.relu(self.gc4(x3_d, adj))\n",
    "        x4_d = F.dropout(x4, self.dropout, training=self.training)\n",
    "        \n",
    "        x5 = self.gc5(x4_d, adj)\n",
    "        return F.log_softmax(x5, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid1=16,\n",
    "            nhid2=16,\n",
    "            nhid3=16,\n",
    "            nhid4=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN1(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nhid3,nhid4,nhid5,nclass, dropout):\n",
    "        super(GCN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc3 = GraphConvolution(nhid2, nhid3,bias=True)\n",
    "        self.gc4 = GraphConvolution(nhid3, nhid4,bias=True)\n",
    "        self.gc5 = GraphConvolution(nhid4, nhid5,bias=True)\n",
    "        self.gc6 = GraphConvolution(nhid5, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = F.relu(self.gc1(x, adj))\n",
    "        x1_d = F.dropout(x1, self.dropout, training=self.training)\n",
    "#         combined = torch.cat([feature, x1_d], dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.gc2(x1_d, adj))\n",
    "        x2_d = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.gc3(x2_d, adj))\n",
    "        x3_d = F.dropout(x3, self.dropout, training=self.training)\n",
    "        \n",
    "        x4 = F.relu(self.gc4(x3_d, adj))\n",
    "        x4_d = F.dropout(x4, self.dropout, training=self.training)\n",
    "        \n",
    "        x5 = F.relu(self.gc5(x4_d, adj))\n",
    "        x5_d = F.dropout(x5, self.dropout, training=self.training)\n",
    "        \n",
    "        x6 = self.gc6(x5_d, adj)\n",
    "        return F.log_softmax(x6, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "for i in range(10):\n",
    "    model1 = GCN1(nfeat=features.shape[1],\n",
    "            nhid1=16,\n",
    "            nhid2=16,\n",
    "            nhid3=16,\n",
    "            nhid4=16,\n",
    "            nhid5=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "    optimizer = optim.Adam(model1.parameters(),\n",
    "                           lr=0.01, weight_decay=5e-4)\n",
    "    t_total = time.time()\n",
    "    record = {}\n",
    "    for epoch in range(200):\n",
    "        train(epoch,model1,record)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    bit_list = sorted(record.keys())\n",
    "    bit_list.reverse()\n",
    "    for key in bit_list[:10]:\n",
    "        value = record[key]\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
